{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd47d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1426a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN='cs-AI'\n",
    "GOOGLE_CLOUD_PROJECT='arxiv-trends'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72888158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabateri/anaconda3/envs/llm-project/lib/python3.11/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "client = bigquery.Client(project=GOOGLE_CLOUD_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b8f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, title, summary, author\n",
    "FROM `arxiv-trends.arxiv_papers.arxiv_papers_2000_2025_cs_AI`\n",
    "WHERE summary IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0726f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/1405.3637v2</td>\n",
       "      <td>Vicious Circle Principle and Logic Programs wi...</td>\n",
       "      <td>The paper presents a knowledge representation ...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/1608.08262v1</td>\n",
       "      <td>Vicious Circle Principle and Formation of Sets...</td>\n",
       "      <td>The paper continues the investigation of Poinc...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/1808.07050v1</td>\n",
       "      <td>Vicious Circle Principle and Logic Programs wi...</td>\n",
       "      <td>The paper presents a knowledge representation ...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2102.04323v2</td>\n",
       "      <td>Discovering a set of policies for the worst ca...</td>\n",
       "      <td>We study the problem of how to construct a set...</td>\n",
       "      <td>[Tom Zahavy, Andre Barreto, Daniel J Mankowitz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2309.13426v2</td>\n",
       "      <td>A Chat About Boring Problems: Studying GPT-bas...</td>\n",
       "      <td>Text normalization - the conversion of text fr...</td>\n",
       "      <td>[Yang Zhang, Travis M. Bartley, Mariana Grater...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109198</th>\n",
       "      <td>http://arxiv.org/abs/2406.11326v1</td>\n",
       "      <td>GitHub Copilot: the perfect Code compLeeter?</td>\n",
       "      <td>This paper aims to evaluate GitHub Copilot's g...</td>\n",
       "      <td>[Ilja Siroš, Dave Singelée, Bart Preneel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109199</th>\n",
       "      <td>http://arxiv.org/abs/physics/0005062v1</td>\n",
       "      <td>Applying MDL to Learning Best Model Granularity</td>\n",
       "      <td>The Minimum Description Length (MDL) principle...</td>\n",
       "      <td>[Qiong Gao, Ming Li, Paul Vitanyi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109200</th>\n",
       "      <td>http://arxiv.org/abs/2202.07290v1</td>\n",
       "      <td>Don't stop the training: continuously-updating...</td>\n",
       "      <td>Over the last decade, numerous studies have sh...</td>\n",
       "      <td>[Pierre Orhan, Yves Boubenec, Jean-Rémi King]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>http://arxiv.org/abs/1911.00572v1</td>\n",
       "      <td>Probabilistic Formulation of the Take The Best...</td>\n",
       "      <td>The framework of cognitively bounded rationali...</td>\n",
       "      <td>[Tomi Peltola, Jussi Jokinen, Samuel Kaski]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109202</th>\n",
       "      <td>http://arxiv.org/abs/2402.09723v3</td>\n",
       "      <td>Efficient Prompt Optimization Through the Lens...</td>\n",
       "      <td>The remarkable instruction-following capabilit...</td>\n",
       "      <td>[Chengshuai Shi, Kun Yang, Zihan Chen, Jundong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109203 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id  \\\n",
       "0             http://arxiv.org/abs/1405.3637v2   \n",
       "1            http://arxiv.org/abs/1608.08262v1   \n",
       "2            http://arxiv.org/abs/1808.07050v1   \n",
       "3            http://arxiv.org/abs/2102.04323v2   \n",
       "4            http://arxiv.org/abs/2309.13426v2   \n",
       "...                                        ...   \n",
       "109198       http://arxiv.org/abs/2406.11326v1   \n",
       "109199  http://arxiv.org/abs/physics/0005062v1   \n",
       "109200       http://arxiv.org/abs/2202.07290v1   \n",
       "109201       http://arxiv.org/abs/1911.00572v1   \n",
       "109202       http://arxiv.org/abs/2402.09723v3   \n",
       "\n",
       "                                                    title  \\\n",
       "0       Vicious Circle Principle and Logic Programs wi...   \n",
       "1       Vicious Circle Principle and Formation of Sets...   \n",
       "2       Vicious Circle Principle and Logic Programs wi...   \n",
       "3       Discovering a set of policies for the worst ca...   \n",
       "4       A Chat About Boring Problems: Studying GPT-bas...   \n",
       "...                                                   ...   \n",
       "109198       GitHub Copilot: the perfect Code compLeeter?   \n",
       "109199    Applying MDL to Learning Best Model Granularity   \n",
       "109200  Don't stop the training: continuously-updating...   \n",
       "109201  Probabilistic Formulation of the Take The Best...   \n",
       "109202  Efficient Prompt Optimization Through the Lens...   \n",
       "\n",
       "                                                  summary  \\\n",
       "0       The paper presents a knowledge representation ...   \n",
       "1       The paper continues the investigation of Poinc...   \n",
       "2       The paper presents a knowledge representation ...   \n",
       "3       We study the problem of how to construct a set...   \n",
       "4       Text normalization - the conversion of text fr...   \n",
       "...                                                   ...   \n",
       "109198  This paper aims to evaluate GitHub Copilot's g...   \n",
       "109199  The Minimum Description Length (MDL) principle...   \n",
       "109200  Over the last decade, numerous studies have sh...   \n",
       "109201  The framework of cognitively bounded rationali...   \n",
       "109202  The remarkable instruction-following capabilit...   \n",
       "\n",
       "                                                   author  \n",
       "0                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "1                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "2                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "3       [Tom Zahavy, Andre Barreto, Daniel J Mankowitz...  \n",
       "4       [Yang Zhang, Travis M. Bartley, Mariana Grater...  \n",
       "...                                                   ...  \n",
       "109198          [Ilja Siroš, Dave Singelée, Bart Preneel]  \n",
       "109199                 [Qiong Gao, Ming Li, Paul Vitanyi]  \n",
       "109200      [Pierre Orhan, Yves Boubenec, Jean-Rémi King]  \n",
       "109201        [Tomi Peltola, Jussi Jokinen, Samuel Kaski]  \n",
       "109202  [Chengshuai Shi, Kun Yang, Zihan Chen, Jundong...  \n",
       "\n",
       "[109203 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbf9abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>http://arxiv.org/abs/2412.13337v1</td>\n",
       "      <td>Unveiling the Secret Recipe: A Guide For Super...</td>\n",
       "      <td>The rise of large language models (LLMs) has c...</td>\n",
       "      <td>[Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "2967  http://arxiv.org/abs/2412.13337v1   \n",
       "\n",
       "                                                  title  \\\n",
       "2967  Unveiling the Secret Recipe: A Guide For Super...   \n",
       "\n",
       "                                                summary  \\\n",
       "2967  The rise of large language models (LLMs) has c...   \n",
       "\n",
       "                                                 author  \n",
       "2967  [Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wan...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['id'] == 'http://arxiv.org/abs/2412.13337v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29bcd3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates: 109203 -> 109203 rows\n"
     ]
    }
   ],
   "source": [
    "# Add this before bulk indexing\n",
    "results_clean = results.drop_duplicates(subset=['id'])\n",
    "print(f\"Removed duplicates: {len(results)} -> {len(results_clean)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a09608",
   "metadata": {},
   "source": [
    "### Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407da148",
   "metadata": {},
   "source": [
    "To use elastic search\n",
    "``` \n",
    "docker run --name es01 --net elastic -p 9200:9200 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.13.4 \n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63667cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to your ES instance\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",  # Or your cloud instance\n",
    "    #basic_auth=(\"user\", \"password\")  # Only if authentication is enabled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4bdf444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c778ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"arxiv-papers\"\n",
    "\n",
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"title\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"summary\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"author\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"published\": {\"type\": \"date\"},\n",
    "            \"categories\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body=index_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee1e906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109203, [])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "def generate_docs(df):\n",
    "    for _, row in df.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"id\": row[\"id\"],\n",
    "            \"_source\": {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"summary\": row[\"summary\"],\n",
    "                \"author\": row[\"author\"],\n",
    "                # \"published\": row[\"published\"].isoformat() if row[\"published\"] else None,\n",
    "                # \"categories\": row[\"categories\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Bulk index all documents\n",
    "bulk(es, generate_docs(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2295f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the latest methods for fine-tuning LLMs on small datasets?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "745963cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(query, top_k=5):\n",
    "    # Text-based search\n",
    "    text_query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"title^2\", \"summary\", \"author\"],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=index_name, body=text_query)\n",
    "    \n",
    "    results = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        results.append({\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"title\": hit[\"_source\"][\"title\"],\n",
    "            \"summary\": hit[\"_source\"][\"summary\"],\n",
    "            \"author\": hit[\"_source\"][\"author\"]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "papers = search_papers(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "llm_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91be2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, top_k=10):\n",
    "    # Search for relevant papers\n",
    "    relevant_papers = search_papers(question, top_k)\n",
    "    \n",
    "    # Build context from summaries\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"id: {paper['id']}\\nPaper: {paper['title']}\\nSummary: {paper['summary']}\"\n",
    "        for paper in relevant_papers\n",
    "    ])\n",
    "    \n",
    "    # Create prompt for LLM\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following research paper summaries, answer the question: {question}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send to your LLM of choice (OpenAI, etc.)\n",
    "    llm_response = llm_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "    return {\n",
    "        \"answer\": llm_response,\n",
    "        \"sources\": relevant_papers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50fcca36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'LLM response here',\n",
       " 'sources': [{'score': 41.224213,\n",
       "   'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "   'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "   'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "   'author': ['Aldo Pareja',\n",
       "    'Nikhil Shivakumar Nayak',\n",
       "    'Hao Wang',\n",
       "    'Krishnateja Killamsetty',\n",
       "    'Shivchander Sudalairaj',\n",
       "    'Wenlong Zhao',\n",
       "    'Seungwook Han',\n",
       "    'Abhishek Bhandwaldar',\n",
       "    'Guangxuan Xu',\n",
       "    'Kai Xu',\n",
       "    'Ligong Han',\n",
       "    'Luke Inglis',\n",
       "    'Akash Srivastava']},\n",
       "  {'score': 41.224213,\n",
       "   'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "   'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "   'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "   'author': ['Aldo Pareja',\n",
       "    'Nikhil Shivakumar Nayak',\n",
       "    'Hao Wang',\n",
       "    'Krishnateja Killamsetty',\n",
       "    'Shivchander Sudalairaj',\n",
       "    'Wenlong Zhao',\n",
       "    'Seungwook Han',\n",
       "    'Abhishek Bhandwaldar',\n",
       "    'Guangxuan Xu',\n",
       "    'Kai Xu',\n",
       "    'Ligong Han',\n",
       "    'Luke Inglis',\n",
       "    'Akash Srivastava']},\n",
       "  {'score': 41.224213,\n",
       "   'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "   'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "   'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "   'author': ['Aldo Pareja',\n",
       "    'Nikhil Shivakumar Nayak',\n",
       "    'Hao Wang',\n",
       "    'Krishnateja Killamsetty',\n",
       "    'Shivchander Sudalairaj',\n",
       "    'Wenlong Zhao',\n",
       "    'Seungwook Han',\n",
       "    'Abhishek Bhandwaldar',\n",
       "    'Guangxuan Xu',\n",
       "    'Kai Xu',\n",
       "    'Ligong Han',\n",
       "    'Luke Inglis',\n",
       "    'Akash Srivastava']},\n",
       "  {'score': 41.224213,\n",
       "   'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "   'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "   'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "   'author': ['Aldo Pareja',\n",
       "    'Nikhil Shivakumar Nayak',\n",
       "    'Hao Wang',\n",
       "    'Krishnateja Killamsetty',\n",
       "    'Shivchander Sudalairaj',\n",
       "    'Wenlong Zhao',\n",
       "    'Seungwook Han',\n",
       "    'Abhishek Bhandwaldar',\n",
       "    'Guangxuan Xu',\n",
       "    'Kai Xu',\n",
       "    'Ligong Han',\n",
       "    'Luke Inglis',\n",
       "    'Akash Srivastava']},\n",
       "  {'score': 41.224213,\n",
       "   'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "   'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "   'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "   'author': ['Aldo Pareja',\n",
       "    'Nikhil Shivakumar Nayak',\n",
       "    'Hao Wang',\n",
       "    'Krishnateja Killamsetty',\n",
       "    'Shivchander Sudalairaj',\n",
       "    'Wenlong Zhao',\n",
       "    'Seungwook Han',\n",
       "    'Abhishek Bhandwaldar',\n",
       "    'Guangxuan Xu',\n",
       "    'Kai Xu',\n",
       "    'Ligong Han',\n",
       "    'Luke Inglis',\n",
       "    'Akash Srivastava']},\n",
       "  {'score': 41.224213,\n",
       "   'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "   'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "   'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "   'author': ['Aldo Pareja',\n",
       "    'Nikhil Shivakumar Nayak',\n",
       "    'Hao Wang',\n",
       "    'Krishnateja Killamsetty',\n",
       "    'Shivchander Sudalairaj',\n",
       "    'Wenlong Zhao',\n",
       "    'Seungwook Han',\n",
       "    'Abhishek Bhandwaldar',\n",
       "    'Guangxuan Xu',\n",
       "    'Kai Xu',\n",
       "    'Ligong Han',\n",
       "    'Luke Inglis',\n",
       "    'Akash Srivastava']},\n",
       "  {'score': 40.626247,\n",
       "   'id': 'http://arxiv.org/abs/2408.11868v1',\n",
       "   'title': 'Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores',\n",
       "   'summary': 'This paper presents an approach to improve text embedding models through\\ncontrastive fine-tuning on small datasets augmented with expert scores. It\\nfocuses on enhancing semantic textual similarity tasks and addressing text\\nretrieval problems. The proposed method uses soft labels derived from\\nexpert-augmented scores to fine-tune embedding models, preserving their\\nversatility and ensuring retrieval capability is improved. The paper evaluates\\nthe method using a Q\\\\&A dataset from an online shopping website and eight\\nexpert models. Results show improved performance over a benchmark model across\\nmultiple metrics on various retrieval tasks from the massive text embedding\\nbenchmark (MTEB). The method is cost-effective and practical for real-world\\napplications, especially when labeled data is scarce.',\n",
       "   'author': ['Jun Lu', 'David Li', 'Bill Ding', 'Yu Kang']},\n",
       "  {'score': 40.626247,\n",
       "   'id': 'http://arxiv.org/abs/2408.11868v1',\n",
       "   'title': 'Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores',\n",
       "   'summary': 'This paper presents an approach to improve text embedding models through\\ncontrastive fine-tuning on small datasets augmented with expert scores. It\\nfocuses on enhancing semantic textual similarity tasks and addressing text\\nretrieval problems. The proposed method uses soft labels derived from\\nexpert-augmented scores to fine-tune embedding models, preserving their\\nversatility and ensuring retrieval capability is improved. The paper evaluates\\nthe method using a Q\\\\&A dataset from an online shopping website and eight\\nexpert models. Results show improved performance over a benchmark model across\\nmultiple metrics on various retrieval tasks from the massive text embedding\\nbenchmark (MTEB). The method is cost-effective and practical for real-world\\napplications, especially when labeled data is scarce.',\n",
       "   'author': ['Jun Lu', 'David Li', 'Bill Ding', 'Yu Kang']},\n",
       "  {'score': 40.626247,\n",
       "   'id': 'http://arxiv.org/abs/2408.11868v1',\n",
       "   'title': 'Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores',\n",
       "   'summary': 'This paper presents an approach to improve text embedding models through\\ncontrastive fine-tuning on small datasets augmented with expert scores. It\\nfocuses on enhancing semantic textual similarity tasks and addressing text\\nretrieval problems. The proposed method uses soft labels derived from\\nexpert-augmented scores to fine-tune embedding models, preserving their\\nversatility and ensuring retrieval capability is improved. The paper evaluates\\nthe method using a Q\\\\&A dataset from an online shopping website and eight\\nexpert models. Results show improved performance over a benchmark model across\\nmultiple metrics on various retrieval tasks from the massive text embedding\\nbenchmark (MTEB). The method is cost-effective and practical for real-world\\napplications, especially when labeled data is scarce.',\n",
       "   'author': ['Jun Lu', 'David Li', 'Bill Ding', 'Yu Kang']},\n",
       "  {'score': 40.626247,\n",
       "   'id': 'http://arxiv.org/abs/2408.11868v1',\n",
       "   'title': 'Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores',\n",
       "   'summary': 'This paper presents an approach to improve text embedding models through\\ncontrastive fine-tuning on small datasets augmented with expert scores. It\\nfocuses on enhancing semantic textual similarity tasks and addressing text\\nretrieval problems. The proposed method uses soft labels derived from\\nexpert-augmented scores to fine-tune embedding models, preserving their\\nversatility and ensuring retrieval capability is improved. The paper evaluates\\nthe method using a Q\\\\&A dataset from an online shopping website and eight\\nexpert models. Results show improved performance over a benchmark model across\\nmultiple metrics on various retrieval tasks from the massive text embedding\\nbenchmark (MTEB). The method is cost-effective and practical for real-world\\napplications, especially when labeled data is scarce.',\n",
       "   'author': ['Jun Lu', 'David Li', 'Bill Ding', 'Yu Kang']}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da9a5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding(text, model=\"o4-mini\"):\n",
    "#     response = openai.Embedding.create(\n",
    "#         input=text,\n",
    "#         model=model\n",
    "#     )\n",
    "#     return response['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f82236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
