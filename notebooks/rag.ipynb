{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd47d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from elasticsearch.helpers import bulk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1426a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN='cs-AI'\n",
    "GOOGLE_CLOUD_PROJECT='arxiv-trends'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbde92",
   "metadata": {},
   "source": [
    "Get environment variables with dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae38d36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ba297",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72888158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabateri/anaconda3/envs/llm-project/lib/python3.11/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "bq_client = bigquery.Client(project=GOOGLE_CLOUD_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bq_data(domain='cs-AI'):\n",
    "    domain_cleaned = domain.replace(\"-\", \"_\")\n",
    "    domain_cleaned = domain_cleaned.replace(\".\", \"_\")\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT id, title, summary, author\n",
    "    FROM `arxiv-trends.arxiv_papers.arxiv_papers_2000_2025_{domain_cleaned}`\n",
    "    WHERE summary IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = bq_client.query(sql_query)\n",
    "    results = query_job.result().to_dataframe()\n",
    "    return results\n",
    "\n",
    "raw_arxiv_df = get_bq_data(domain=DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0726f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/1405.3637v2</td>\n",
       "      <td>Vicious Circle Principle and Logic Programs wi...</td>\n",
       "      <td>The paper presents a knowledge representation ...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/1608.08262v1</td>\n",
       "      <td>Vicious Circle Principle and Formation of Sets...</td>\n",
       "      <td>The paper continues the investigation of Poinc...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/1808.07050v1</td>\n",
       "      <td>Vicious Circle Principle and Logic Programs wi...</td>\n",
       "      <td>The paper presents a knowledge representation ...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2102.04323v2</td>\n",
       "      <td>Discovering a set of policies for the worst ca...</td>\n",
       "      <td>We study the problem of how to construct a set...</td>\n",
       "      <td>[Tom Zahavy, Andre Barreto, Daniel J Mankowitz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2309.13426v2</td>\n",
       "      <td>A Chat About Boring Problems: Studying GPT-bas...</td>\n",
       "      <td>Text normalization - the conversion of text fr...</td>\n",
       "      <td>[Yang Zhang, Travis M. Bartley, Mariana Grater...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109198</th>\n",
       "      <td>http://arxiv.org/abs/2406.11326v1</td>\n",
       "      <td>GitHub Copilot: the perfect Code compLeeter?</td>\n",
       "      <td>This paper aims to evaluate GitHub Copilot's g...</td>\n",
       "      <td>[Ilja Siroš, Dave Singelée, Bart Preneel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109199</th>\n",
       "      <td>http://arxiv.org/abs/physics/0005062v1</td>\n",
       "      <td>Applying MDL to Learning Best Model Granularity</td>\n",
       "      <td>The Minimum Description Length (MDL) principle...</td>\n",
       "      <td>[Qiong Gao, Ming Li, Paul Vitanyi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109200</th>\n",
       "      <td>http://arxiv.org/abs/2202.07290v1</td>\n",
       "      <td>Don't stop the training: continuously-updating...</td>\n",
       "      <td>Over the last decade, numerous studies have sh...</td>\n",
       "      <td>[Pierre Orhan, Yves Boubenec, Jean-Rémi King]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>http://arxiv.org/abs/1911.00572v1</td>\n",
       "      <td>Probabilistic Formulation of the Take The Best...</td>\n",
       "      <td>The framework of cognitively bounded rationali...</td>\n",
       "      <td>[Tomi Peltola, Jussi Jokinen, Samuel Kaski]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109202</th>\n",
       "      <td>http://arxiv.org/abs/2402.09723v3</td>\n",
       "      <td>Efficient Prompt Optimization Through the Lens...</td>\n",
       "      <td>The remarkable instruction-following capabilit...</td>\n",
       "      <td>[Chengshuai Shi, Kun Yang, Zihan Chen, Jundong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109203 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id  \\\n",
       "0             http://arxiv.org/abs/1405.3637v2   \n",
       "1            http://arxiv.org/abs/1608.08262v1   \n",
       "2            http://arxiv.org/abs/1808.07050v1   \n",
       "3            http://arxiv.org/abs/2102.04323v2   \n",
       "4            http://arxiv.org/abs/2309.13426v2   \n",
       "...                                        ...   \n",
       "109198       http://arxiv.org/abs/2406.11326v1   \n",
       "109199  http://arxiv.org/abs/physics/0005062v1   \n",
       "109200       http://arxiv.org/abs/2202.07290v1   \n",
       "109201       http://arxiv.org/abs/1911.00572v1   \n",
       "109202       http://arxiv.org/abs/2402.09723v3   \n",
       "\n",
       "                                                    title  \\\n",
       "0       Vicious Circle Principle and Logic Programs wi...   \n",
       "1       Vicious Circle Principle and Formation of Sets...   \n",
       "2       Vicious Circle Principle and Logic Programs wi...   \n",
       "3       Discovering a set of policies for the worst ca...   \n",
       "4       A Chat About Boring Problems: Studying GPT-bas...   \n",
       "...                                                   ...   \n",
       "109198       GitHub Copilot: the perfect Code compLeeter?   \n",
       "109199    Applying MDL to Learning Best Model Granularity   \n",
       "109200  Don't stop the training: continuously-updating...   \n",
       "109201  Probabilistic Formulation of the Take The Best...   \n",
       "109202  Efficient Prompt Optimization Through the Lens...   \n",
       "\n",
       "                                                  summary  \\\n",
       "0       The paper presents a knowledge representation ...   \n",
       "1       The paper continues the investigation of Poinc...   \n",
       "2       The paper presents a knowledge representation ...   \n",
       "3       We study the problem of how to construct a set...   \n",
       "4       Text normalization - the conversion of text fr...   \n",
       "...                                                   ...   \n",
       "109198  This paper aims to evaluate GitHub Copilot's g...   \n",
       "109199  The Minimum Description Length (MDL) principle...   \n",
       "109200  Over the last decade, numerous studies have sh...   \n",
       "109201  The framework of cognitively bounded rationali...   \n",
       "109202  The remarkable instruction-following capabilit...   \n",
       "\n",
       "                                                   author  \n",
       "0                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "1                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "2                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "3       [Tom Zahavy, Andre Barreto, Daniel J Mankowitz...  \n",
       "4       [Yang Zhang, Travis M. Bartley, Mariana Grater...  \n",
       "...                                                   ...  \n",
       "109198          [Ilja Siroš, Dave Singelée, Bart Preneel]  \n",
       "109199                 [Qiong Gao, Ming Li, Paul Vitanyi]  \n",
       "109200      [Pierre Orhan, Yves Boubenec, Jean-Rémi King]  \n",
       "109201        [Tomi Peltola, Jussi Jokinen, Samuel Kaski]  \n",
       "109202  [Chengshuai Shi, Kun Yang, Zihan Chen, Jundong...  \n",
       "\n",
       "[109203 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_arxiv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf9abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>http://arxiv.org/abs/2412.13337v1</td>\n",
       "      <td>Unveiling the Secret Recipe: A Guide For Super...</td>\n",
       "      <td>The rise of large language models (LLMs) has c...</td>\n",
       "      <td>[Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "2967  http://arxiv.org/abs/2412.13337v1   \n",
       "\n",
       "                                                  title  \\\n",
       "2967  Unveiling the Secret Recipe: A Guide For Super...   \n",
       "\n",
       "                                                summary  \\\n",
       "2967  The rise of large language models (LLMs) has c...   \n",
       "\n",
       "                                                 author  \n",
       "2967  [Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wan...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_arxiv_df[raw_arxiv_df['id'] == 'http://arxiv.org/abs/2412.13337v1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25bb4d",
   "metadata": {},
   "source": [
    "Remove duplicates (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcd3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates: 109203 -> 109203 rows\n"
     ]
    }
   ],
   "source": [
    "# Clean duplicates before bulk indexing\n",
    "arxiv_df = raw_arxiv_df.drop_duplicates(subset=['id'])\n",
    "print(f\"Removed duplicates: {len(raw_arxiv_df)} -> {len(arxiv_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a09608",
   "metadata": {},
   "source": [
    "### Elastic Search: Find the most relevant papers for a given query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407da148",
   "metadata": {},
   "source": [
    "To use elastic search\n",
    "``` \n",
    "docker run --name es01 --net elastic -p 9200:9200 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.13.4 \n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63667cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to your ES instance\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",  # Or your cloud instance\n",
    "    #basic_auth=(\"user\", \"password\")  # Only if authentication is enabled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4bdf444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c778ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"arxiv-papers\"\n",
    "\n",
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"title\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"summary\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"author\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            #\"published\": {\"type\": \"date\"},\n",
    "            #\"categories\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body=index_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docs(df):\n",
    "    for _, row in df.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": row[\"id\"],\n",
    "            \"_source\": {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"summary\": row[\"summary\"],\n",
    "                \"author\": row[\"author\"],\n",
    "                # \"published\": row[\"published\"].isoformat() if row[\"published\"] else None,\n",
    "                # \"categories\": row[\"categories\"]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745963cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(query, top_k=10):\n",
    "    # Text-based search\n",
    "    text_query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"title^2\", \"summary\"],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=index_name, body=text_query)\n",
    "    \n",
    "    results = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        results.append({\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"title\": hit[\"_source\"][\"title\"],\n",
    "            \"summary\": hit[\"_source\"][\"summary\"],\n",
    "            \"author\": hit[\"_source\"][\"author\"]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab48dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109203, [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bulk index all documents\n",
    "bulk(es, generate_docs(arxiv_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b25ad",
   "metadata": {},
   "source": [
    "### Using a LLM to asnwer a query based on the most relevant papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5c0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "llm_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a0d1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, relevant_papers):\n",
    "    # Build context from summaries\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"id: {paper['id']}\\nPaper: {paper['title']}\\nSummary: {paper['summary']}\"\n",
    "        for paper in relevant_papers\n",
    "    ])\n",
    "    \n",
    "    # Create prompt for LLM\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following research paper summaries, answer the question: {query}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f91be2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, relevant_papers, model):\n",
    "    \n",
    "    # Send to your LLM of choice (OpenAI, etc.)\n",
    "    llm_response = llm_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "    return {\n",
    "        \"llm_answer\": llm_response,\n",
    "        \"sources\": relevant_papers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50fcca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, top_k, model):\n",
    "    # search top N papers using elastic search\n",
    "    relevant_papers = search_papers(query, top_k=top_k)\n",
    "\n",
    "    # build prompt\n",
    "    prompt = build_prompt(query, relevant_papers)\n",
    "\n",
    "    # generate llm answer based on the relevant papers\n",
    "    answer = llm(prompt, relevant_papers=relevant_papers, model=model)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c717b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the latest methods for fine-tuning LLMs on small datasets?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff4c5c",
   "metadata": {},
   "source": [
    "Look for the most relevant papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d38f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 41.763905,\n",
       "  'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "  'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "  'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "  'author': ['Aldo Pareja',\n",
       "   'Nikhil Shivakumar Nayak',\n",
       "   'Hao Wang',\n",
       "   'Krishnateja Killamsetty',\n",
       "   'Shivchander Sudalairaj',\n",
       "   'Wenlong Zhao',\n",
       "   'Seungwook Han',\n",
       "   'Abhishek Bhandwaldar',\n",
       "   'Guangxuan Xu',\n",
       "   'Kai Xu',\n",
       "   'Ligong Han',\n",
       "   'Luke Inglis',\n",
       "   'Akash Srivastava']},\n",
       " {'score': 41.217094,\n",
       "  'id': 'http://arxiv.org/abs/2408.11868v1',\n",
       "  'title': 'Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores',\n",
       "  'summary': 'This paper presents an approach to improve text embedding models through\\ncontrastive fine-tuning on small datasets augmented with expert scores. It\\nfocuses on enhancing semantic textual similarity tasks and addressing text\\nretrieval problems. The proposed method uses soft labels derived from\\nexpert-augmented scores to fine-tune embedding models, preserving their\\nversatility and ensuring retrieval capability is improved. The paper evaluates\\nthe method using a Q\\\\&A dataset from an online shopping website and eight\\nexpert models. Results show improved performance over a benchmark model across\\nmultiple metrics on various retrieval tasks from the massive text embedding\\nbenchmark (MTEB). The method is cost-effective and practical for real-world\\napplications, especially when labeled data is scarce.',\n",
       "  'author': ['Jun Lu', 'David Li', 'Bill Ding', 'Yu Kang']},\n",
       " {'score': 38.866642,\n",
       "  'id': 'http://arxiv.org/abs/1609.02531v1',\n",
       "  'title': 'Latest Datasets and Technologies Presented in the Workshop on Grasping and Manipulation Datasets',\n",
       "  'summary': 'This paper reports the activities and outcomes in the Workshop on Grasping\\nand Manipulation Datasets that was organized under the International Conference\\non Robotics and Automation (ICRA) 2016. The half day workshop was packed with\\nnine invited talks, 12 interactive presentations, and one panel discussion with\\nten panelists. This paper summarizes all the talks and presentations and recaps\\nwhat has been discussed in the panels session. This summary servers as a review\\nof recent developments in data collection in grasping and manipulation. Many of\\nthe presentations describe ongoing efforts or explorations that could be\\nachieved and fully available in a year or two. The panel discussion not only\\ncommented on the current approaches, but also indicates new directions and\\nfocuses. The workshop clearly displayed the importance of quality datasets in\\nrobotics and robotic grasping and manipulation field. Hopefully the workshop\\ncould motivate larger efforts to create big datasets that are comparable with\\nbig datasets in other communities such as computer vision.',\n",
       "  'author': ['Matteo Bianchi', 'Jeannette Bohg', 'Yu Sun']},\n",
       " {'score': 38.350567,\n",
       "  'id': 'http://arxiv.org/abs/2407.02960v2',\n",
       "  'title': 'ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets',\n",
       "  'summary': 'This work addresses the timely yet underexplored problem of performing\\ninference and finetuning of a proprietary LLM owned by a model provider entity\\non the confidential/private data of another data owner entity, in a way that\\nensures the confidentiality of both the model and the data. Hereby, the\\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\\nnovel, efficient and fully utility-preserving approach that combines a simple\\nyet effective obfuscation technique with an efficient usage of confidential\\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\\nna\\\\\"ive version of our approach to highlight the necessity of using random\\nmatrices with low condition numbers in our approach to reduce errors induced by\\nthe obfuscation.',\n",
       "  'author': ['Ahmed Frikha',\n",
       "   'Nassim Walha',\n",
       "   'Ricardo Mendes',\n",
       "   'Krishna Kanth Nakka',\n",
       "   'Xue Jiang',\n",
       "   'Xuebing Zhou']},\n",
       " {'score': 38.318386,\n",
       "  'id': 'http://arxiv.org/abs/2406.04383v2',\n",
       "  'title': 'Exploring the Latest LLMs for Leaderboard Extraction',\n",
       "  'summary': 'The rapid advancements in Large Language Models (LLMs) have opened new\\navenues for automating complex tasks in AI research. This paper investigates\\nthe efficacy of different LLMs-Mistral 7B, Llama-2, GPT-4-Turbo and GPT-4.o in\\nextracting leaderboard information from empirical AI research articles. We\\nexplore three types of contextual inputs to the models: DocTAET (Document\\nTitle, Abstract, Experimental Setup, and Tabular Information), DocREC (Results,\\nExperiments, and Conclusions), and DocFULL (entire document). Our comprehensive\\nstudy evaluates the performance of these models in generating (Task, Dataset,\\nMetric, Score) quadruples from research papers. The findings reveal significant\\ninsights into the strengths and limitations of each model and context type,\\nproviding valuable guidance for future AI research automation efforts.',\n",
       "  'author': ['Salomon Kabongo', \"Jennifer D'Souza\", 'Sören Auer']}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(query,top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719952e",
   "metadata": {},
   "source": [
    "Use an LLM to give an answer using as a context the most relevant papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f82236",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag(query,top_k=5,model=\"o4-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bdb7b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three main strands of very recent work on “small-data” fine-tuning of LLMs can be grouped as follows:\n",
      "\n",
      "1.  Supervised instruction-tuning of small (3 B–7 B) LLMs with hyper-parameter best-practices  \n",
      "    •  Large batch sizes + low learning rates often outperform the more common small-batch/high-LR recipes.  \n",
      "    •  Monitor early-stage training dynamics (gradient norms, loss curves) to kill bad runs and save computation.  \n",
      "    •  Simple learning-rate schedules and reduced warm-up are sufficient—no need for elaborate phased schedules.  \n",
      "    •  “Stacked” instruction mixing (train on all tasks at once) is as good as or better than multi-phase curricula, and is easier to implement.\n",
      "\n",
      "2.  Contrastive fine-tuning of embeddings on tiny labeled sets  \n",
      "    •  Build anchor/positive/negative pairs and use a contrastive loss to sharpen semantic similarity.  \n",
      "    •  Augment your small corpus with soft/expert-provided similarity scores so that the model “knows” graded relevance.  \n",
      "    •  This yields strong retrieval and downstream performance on tasks like Q & A or text search even when you only have hundreds of examples.\n",
      "\n",
      "3.  Privacy-preserving off-site fine-tuning of proprietary LLMs (ObfuscaTune)  \n",
      "    •  Model owner and data owner never see each other’s assets in the clear.  \n",
      "    •  Apply a light obfuscation (random low-condition matrices) to the model weights before sending them to the cloud.  \n",
      "    •  Perform the bulk of training off-site in standard compute, and confine only a small fraction (≈5 %) of parameters inside a trusted execution environment (TEE).  \n",
      "    •  At no point are the raw model weights or private data exposed, yet full fine-tuning utility is preserved.\n",
      "\n",
      "Taken together, these methods let practitioners with limited data and compute  (or with strict privacy requirements) successfully adapt LLMs.\n"
     ]
    }
   ],
   "source": [
    "print(answer['llm_answer'].choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abbb8042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_answer': ChatCompletion(id='chatcmpl-BpZ6WAKnXiWbGpbFFMxFkjopAlqQc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The three main strands of very recent work on “small-data” fine-tuning of LLMs can be grouped as follows:\\n\\n1.  Supervised instruction-tuning of small (3\\u2009B–7\\u2009B) LLMs with hyper-parameter best-practices  \\n    •  Large batch sizes + low learning rates often outperform the more common small-batch/high-LR recipes.  \\n    •  Monitor early-stage training dynamics (gradient norms, loss curves) to kill bad runs and save computation.  \\n    •  Simple learning-rate schedules and reduced warm-up are sufficient—no need for elaborate phased schedules.  \\n    •  “Stacked” instruction mixing (train on all tasks at once) is as good as or better than multi-phase curricula, and is easier to implement.\\n\\n2.  Contrastive fine-tuning of embeddings on tiny labeled sets  \\n    •  Build anchor/positive/negative pairs and use a contrastive loss to sharpen semantic similarity.  \\n    •  Augment your small corpus with soft/expert-provided similarity scores so that the model “knows” graded relevance.  \\n    •  This yields strong retrieval and downstream performance on tasks like Q\\u2009&\\u2009A or text search even when you only have hundreds of examples.\\n\\n3.  Privacy-preserving off-site fine-tuning of proprietary LLMs (ObfuscaTune)  \\n    •  Model owner and data owner never see each other’s assets in the clear.  \\n    •  Apply a light obfuscation (random low-condition matrices) to the model weights before sending them to the cloud.  \\n    •  Perform the bulk of training off-site in standard compute, and confine only a small fraction (≈5\\u2009%) of parameters inside a trusted execution environment (TEE).  \\n    •  At no point are the raw model weights or private data exposed, yet full fine-tuning utility is preserved.\\n\\nTaken together, these methods let practitioners with limited data and compute  (or with strict privacy requirements) successfully adapt LLMs.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1751629368, model='o4-mini-2025-04-16', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1342, prompt_tokens=1374, total_tokens=2716, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=896, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1280))),\n",
       " 'sources': [{'score': 41.763905,\n",
       "   'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "   'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "   'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "   'author': ['Aldo Pareja',\n",
       "    'Nikhil Shivakumar Nayak',\n",
       "    'Hao Wang',\n",
       "    'Krishnateja Killamsetty',\n",
       "    'Shivchander Sudalairaj',\n",
       "    'Wenlong Zhao',\n",
       "    'Seungwook Han',\n",
       "    'Abhishek Bhandwaldar',\n",
       "    'Guangxuan Xu',\n",
       "    'Kai Xu',\n",
       "    'Ligong Han',\n",
       "    'Luke Inglis',\n",
       "    'Akash Srivastava']},\n",
       "  {'score': 41.217094,\n",
       "   'id': 'http://arxiv.org/abs/2408.11868v1',\n",
       "   'title': 'Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores',\n",
       "   'summary': 'This paper presents an approach to improve text embedding models through\\ncontrastive fine-tuning on small datasets augmented with expert scores. It\\nfocuses on enhancing semantic textual similarity tasks and addressing text\\nretrieval problems. The proposed method uses soft labels derived from\\nexpert-augmented scores to fine-tune embedding models, preserving their\\nversatility and ensuring retrieval capability is improved. The paper evaluates\\nthe method using a Q\\\\&A dataset from an online shopping website and eight\\nexpert models. Results show improved performance over a benchmark model across\\nmultiple metrics on various retrieval tasks from the massive text embedding\\nbenchmark (MTEB). The method is cost-effective and practical for real-world\\napplications, especially when labeled data is scarce.',\n",
       "   'author': ['Jun Lu', 'David Li', 'Bill Ding', 'Yu Kang']},\n",
       "  {'score': 38.866642,\n",
       "   'id': 'http://arxiv.org/abs/1609.02531v1',\n",
       "   'title': 'Latest Datasets and Technologies Presented in the Workshop on Grasping and Manipulation Datasets',\n",
       "   'summary': 'This paper reports the activities and outcomes in the Workshop on Grasping\\nand Manipulation Datasets that was organized under the International Conference\\non Robotics and Automation (ICRA) 2016. The half day workshop was packed with\\nnine invited talks, 12 interactive presentations, and one panel discussion with\\nten panelists. This paper summarizes all the talks and presentations and recaps\\nwhat has been discussed in the panels session. This summary servers as a review\\nof recent developments in data collection in grasping and manipulation. Many of\\nthe presentations describe ongoing efforts or explorations that could be\\nachieved and fully available in a year or two. The panel discussion not only\\ncommented on the current approaches, but also indicates new directions and\\nfocuses. The workshop clearly displayed the importance of quality datasets in\\nrobotics and robotic grasping and manipulation field. Hopefully the workshop\\ncould motivate larger efforts to create big datasets that are comparable with\\nbig datasets in other communities such as computer vision.',\n",
       "   'author': ['Matteo Bianchi', 'Jeannette Bohg', 'Yu Sun']},\n",
       "  {'score': 38.350567,\n",
       "   'id': 'http://arxiv.org/abs/2407.02960v2',\n",
       "   'title': 'ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets',\n",
       "   'summary': 'This work addresses the timely yet underexplored problem of performing\\ninference and finetuning of a proprietary LLM owned by a model provider entity\\non the confidential/private data of another data owner entity, in a way that\\nensures the confidentiality of both the model and the data. Hereby, the\\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\\nnovel, efficient and fully utility-preserving approach that combines a simple\\nyet effective obfuscation technique with an efficient usage of confidential\\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\\nna\\\\\"ive version of our approach to highlight the necessity of using random\\nmatrices with low condition numbers in our approach to reduce errors induced by\\nthe obfuscation.',\n",
       "   'author': ['Ahmed Frikha',\n",
       "    'Nassim Walha',\n",
       "    'Ricardo Mendes',\n",
       "    'Krishna Kanth Nakka',\n",
       "    'Xue Jiang',\n",
       "    'Xuebing Zhou']},\n",
       "  {'score': 38.318386,\n",
       "   'id': 'http://arxiv.org/abs/2406.04383v2',\n",
       "   'title': 'Exploring the Latest LLMs for Leaderboard Extraction',\n",
       "   'summary': 'The rapid advancements in Large Language Models (LLMs) have opened new\\navenues for automating complex tasks in AI research. This paper investigates\\nthe efficacy of different LLMs-Mistral 7B, Llama-2, GPT-4-Turbo and GPT-4.o in\\nextracting leaderboard information from empirical AI research articles. We\\nexplore three types of contextual inputs to the models: DocTAET (Document\\nTitle, Abstract, Experimental Setup, and Tabular Information), DocREC (Results,\\nExperiments, and Conclusions), and DocFULL (entire document). Our comprehensive\\nstudy evaluates the performance of these models in generating (Task, Dataset,\\nMetric, Score) quadruples from research papers. The findings reveal significant\\ninsights into the strengths and limitations of each model and context type,\\nproviding valuable guidance for future AI research automation efforts.',\n",
       "   'author': ['Salomon Kabongo', \"Jennifer D'Souza\", 'Sören Auer']}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766584e",
   "metadata": {},
   "source": [
    "### RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd464fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@1: 0.16\n",
      "Hit@5: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Load your generated questions\n",
    "eval_df = pd.read_csv('arxiv_ground_truth_retrieval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "529a1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your RAG system\n",
    "def evaluate_rag(question, expected_paper_id):\n",
    "    # Run your RAG\n",
    "    results = search_papers(question, top_k=5)\n",
    "    \n",
    "    # Check if expected paper is in top results\n",
    "    retrieved_ids = [r['id'] for r in results]\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'expected_paper': expected_paper_id,\n",
    "        'retrieved_papers': retrieved_ids,\n",
    "        'hit_at_1': expected_paper_id == retrieved_ids[0] if retrieved_ids else False,\n",
    "        'hit_at_5': expected_paper_id in retrieved_ids\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "385cb689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@1: 0.16\n",
      "Hit@5: 0.40\n"
     ]
    }
   ],
   "source": [
    "sample_size=50\n",
    "# Evaluate on sample\n",
    "sample_questions = eval_df.head(sample_size)\n",
    "eval_results = []\n",
    "\n",
    "for _, row in sample_questions.iterrows():\n",
    "    result = evaluate_rag(row['question'], row['paper_id'])\n",
    "    eval_results.append(result)\n",
    "\n",
    "# Calculate metrics\n",
    "hit_at_1 = sum([r['hit_at_1'] for r in eval_results]) / len(eval_results)\n",
    "hit_at_5 = sum([r['hit_at_5'] for r in eval_results]) / len(eval_results)\n",
    "\n",
    "print(f\"Hit@1: {hit_at_1:.2f}\")\n",
    "print(f\"Hit@5: {hit_at_5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6def3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
