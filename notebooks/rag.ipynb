{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd47d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from elasticsearch.helpers import bulk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1426a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN='cs-AI'\n",
    "GOOGLE_CLOUD_PROJECT='arxiv-trends'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbde92",
   "metadata": {},
   "source": [
    "Get environment variables with dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae38d36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ba297",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72888158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabateri/anaconda3/envs/llm-project/lib/python3.11/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "bq_client = bigquery.Client(project=GOOGLE_CLOUD_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bq_data(domain='cs-AI'):\n",
    "    domain_cleaned = domain.replace(\"-\", \"_\")\n",
    "    domain_cleaned = domain_cleaned.replace(\".\", \"_\")\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT id, title, summary, author\n",
    "    FROM `arxiv-trends.arxiv_papers.arxiv_papers_2000_2025_{domain_cleaned}`\n",
    "    WHERE summary IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = bq_client.query(sql_query)\n",
    "    results = query_job.result().to_dataframe()\n",
    "    return results\n",
    "\n",
    "raw_arxiv_df = get_bq_data(domain=DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0726f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/1405.3637v2</td>\n",
       "      <td>Vicious Circle Principle and Logic Programs wi...</td>\n",
       "      <td>The paper presents a knowledge representation ...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/1608.08262v1</td>\n",
       "      <td>Vicious Circle Principle and Formation of Sets...</td>\n",
       "      <td>The paper continues the investigation of Poinc...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/1808.07050v1</td>\n",
       "      <td>Vicious Circle Principle and Logic Programs wi...</td>\n",
       "      <td>The paper presents a knowledge representation ...</td>\n",
       "      <td>[Michael Gelfond, Yuanlin Zhang]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2102.04323v2</td>\n",
       "      <td>Discovering a set of policies for the worst ca...</td>\n",
       "      <td>We study the problem of how to construct a set...</td>\n",
       "      <td>[Tom Zahavy, Andre Barreto, Daniel J Mankowitz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2309.13426v2</td>\n",
       "      <td>A Chat About Boring Problems: Studying GPT-bas...</td>\n",
       "      <td>Text normalization - the conversion of text fr...</td>\n",
       "      <td>[Yang Zhang, Travis M. Bartley, Mariana Grater...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109198</th>\n",
       "      <td>http://arxiv.org/abs/2406.11326v1</td>\n",
       "      <td>GitHub Copilot: the perfect Code compLeeter?</td>\n",
       "      <td>This paper aims to evaluate GitHub Copilot's g...</td>\n",
       "      <td>[Ilja Siroš, Dave Singelée, Bart Preneel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109199</th>\n",
       "      <td>http://arxiv.org/abs/physics/0005062v1</td>\n",
       "      <td>Applying MDL to Learning Best Model Granularity</td>\n",
       "      <td>The Minimum Description Length (MDL) principle...</td>\n",
       "      <td>[Qiong Gao, Ming Li, Paul Vitanyi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109200</th>\n",
       "      <td>http://arxiv.org/abs/2202.07290v1</td>\n",
       "      <td>Don't stop the training: continuously-updating...</td>\n",
       "      <td>Over the last decade, numerous studies have sh...</td>\n",
       "      <td>[Pierre Orhan, Yves Boubenec, Jean-Rémi King]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>http://arxiv.org/abs/1911.00572v1</td>\n",
       "      <td>Probabilistic Formulation of the Take The Best...</td>\n",
       "      <td>The framework of cognitively bounded rationali...</td>\n",
       "      <td>[Tomi Peltola, Jussi Jokinen, Samuel Kaski]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109202</th>\n",
       "      <td>http://arxiv.org/abs/2402.09723v3</td>\n",
       "      <td>Efficient Prompt Optimization Through the Lens...</td>\n",
       "      <td>The remarkable instruction-following capabilit...</td>\n",
       "      <td>[Chengshuai Shi, Kun Yang, Zihan Chen, Jundong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109203 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id  \\\n",
       "0             http://arxiv.org/abs/1405.3637v2   \n",
       "1            http://arxiv.org/abs/1608.08262v1   \n",
       "2            http://arxiv.org/abs/1808.07050v1   \n",
       "3            http://arxiv.org/abs/2102.04323v2   \n",
       "4            http://arxiv.org/abs/2309.13426v2   \n",
       "...                                        ...   \n",
       "109198       http://arxiv.org/abs/2406.11326v1   \n",
       "109199  http://arxiv.org/abs/physics/0005062v1   \n",
       "109200       http://arxiv.org/abs/2202.07290v1   \n",
       "109201       http://arxiv.org/abs/1911.00572v1   \n",
       "109202       http://arxiv.org/abs/2402.09723v3   \n",
       "\n",
       "                                                    title  \\\n",
       "0       Vicious Circle Principle and Logic Programs wi...   \n",
       "1       Vicious Circle Principle and Formation of Sets...   \n",
       "2       Vicious Circle Principle and Logic Programs wi...   \n",
       "3       Discovering a set of policies for the worst ca...   \n",
       "4       A Chat About Boring Problems: Studying GPT-bas...   \n",
       "...                                                   ...   \n",
       "109198       GitHub Copilot: the perfect Code compLeeter?   \n",
       "109199    Applying MDL to Learning Best Model Granularity   \n",
       "109200  Don't stop the training: continuously-updating...   \n",
       "109201  Probabilistic Formulation of the Take The Best...   \n",
       "109202  Efficient Prompt Optimization Through the Lens...   \n",
       "\n",
       "                                                  summary  \\\n",
       "0       The paper presents a knowledge representation ...   \n",
       "1       The paper continues the investigation of Poinc...   \n",
       "2       The paper presents a knowledge representation ...   \n",
       "3       We study the problem of how to construct a set...   \n",
       "4       Text normalization - the conversion of text fr...   \n",
       "...                                                   ...   \n",
       "109198  This paper aims to evaluate GitHub Copilot's g...   \n",
       "109199  The Minimum Description Length (MDL) principle...   \n",
       "109200  Over the last decade, numerous studies have sh...   \n",
       "109201  The framework of cognitively bounded rationali...   \n",
       "109202  The remarkable instruction-following capabilit...   \n",
       "\n",
       "                                                   author  \n",
       "0                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "1                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "2                        [Michael Gelfond, Yuanlin Zhang]  \n",
       "3       [Tom Zahavy, Andre Barreto, Daniel J Mankowitz...  \n",
       "4       [Yang Zhang, Travis M. Bartley, Mariana Grater...  \n",
       "...                                                   ...  \n",
       "109198          [Ilja Siroš, Dave Singelée, Bart Preneel]  \n",
       "109199                 [Qiong Gao, Ming Li, Paul Vitanyi]  \n",
       "109200      [Pierre Orhan, Yves Boubenec, Jean-Rémi King]  \n",
       "109201        [Tomi Peltola, Jussi Jokinen, Samuel Kaski]  \n",
       "109202  [Chengshuai Shi, Kun Yang, Zihan Chen, Jundong...  \n",
       "\n",
       "[109203 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_arxiv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf9abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>http://arxiv.org/abs/2412.13337v1</td>\n",
       "      <td>Unveiling the Secret Recipe: A Guide For Super...</td>\n",
       "      <td>The rise of large language models (LLMs) has c...</td>\n",
       "      <td>[Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "2967  http://arxiv.org/abs/2412.13337v1   \n",
       "\n",
       "                                                  title  \\\n",
       "2967  Unveiling the Secret Recipe: A Guide For Super...   \n",
       "\n",
       "                                                summary  \\\n",
       "2967  The rise of large language models (LLMs) has c...   \n",
       "\n",
       "                                                 author  \n",
       "2967  [Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wan...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_arxiv_df[raw_arxiv_df['id'] == 'http://arxiv.org/abs/2412.13337v1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25bb4d",
   "metadata": {},
   "source": [
    "Remove duplicates (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29bcd3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates: 109203 -> 109203 rows\n"
     ]
    }
   ],
   "source": [
    "# Clean duplicates before bulk indexing\n",
    "arxiv_df = raw_arxiv_df.drop_duplicates(subset=['id'])\n",
    "print(f\"Removed duplicates: {len(raw_arxiv_df)} -> {len(arxiv_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a09608",
   "metadata": {},
   "source": [
    "### Elastic Search: Find the most relevant papers for a given query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407da148",
   "metadata": {},
   "source": [
    "To use elastic search\n",
    "``` \n",
    "docker run --name es01 --net elastic -p 9200:9200 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.13.4 \n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63667cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to your ES instance\n",
    "es = Elasticsearch(\n",
    "    \"http://localhost:9200\",  # Or your cloud instance\n",
    "    #basic_auth=(\"user\", \"password\")  # Only if authentication is enabled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4bdf444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(es.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c778ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"arxiv-papers\"\n",
    "\n",
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"title\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"summary\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            \"author\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "            #\"published\": {\"type\": \"date\"},\n",
    "            #\"categories\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body=index_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docs(df):\n",
    "    for _, row in df.iterrows():\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": row[\"id\"],\n",
    "            \"_source\": {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"summary\": row[\"summary\"],\n",
    "                \"author\": row[\"author\"],\n",
    "                # \"published\": row[\"published\"].isoformat() if row[\"published\"] else None,\n",
    "                # \"categories\": row[\"categories\"]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "745963cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(query, top_k=10):\n",
    "    # Text-based search\n",
    "    text_query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"title^0.5\", \"summary\"],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=index_name, body=text_query)\n",
    "    \n",
    "    results = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        results.append({\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"title\": hit[\"_source\"][\"title\"],\n",
    "            \"summary\": hit[\"_source\"][\"summary\"],\n",
    "            \"author\": hit[\"_source\"][\"author\"]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ab48dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109203, [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bulk index all documents\n",
    "bulk(es, generate_docs(arxiv_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b25ad",
   "metadata": {},
   "source": [
    "### Using a LLM to asnwer a query based on the most relevant papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5c0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "llm_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a0d1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, relevant_papers):\n",
    "    # Build context from summaries\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"id: {paper['id']}\\nPaper: {paper['title']}\\nSummary: {paper['summary']}\"\n",
    "        for paper in relevant_papers\n",
    "    ])\n",
    "    \n",
    "    # Create prompt for LLM\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following research paper summaries, answer the question: {query}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f91be2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, relevant_papers, model):\n",
    "    \n",
    "    # Send to your LLM of choice (OpenAI, etc.)\n",
    "    llm_response = llm_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "    return {\n",
    "        \"llm_answer\": llm_response,\n",
    "        \"sources\": relevant_papers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50fcca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, top_k, model):\n",
    "    # search top N papers using elastic search\n",
    "    relevant_papers = search_papers(query, top_k=top_k)\n",
    "\n",
    "    # build prompt\n",
    "    prompt = build_prompt(query, relevant_papers)\n",
    "\n",
    "    # generate llm answer based on the relevant papers\n",
    "    answer = llm(prompt, relevant_papers=relevant_papers, model=model)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c717b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the latest methods for fine-tuning LLMs on small datasets?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff4c5c",
   "metadata": {},
   "source": [
    "Look for the most relevant papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d38f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 41.763905,\n",
       "  'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "  'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "  'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "  'author': ['Aldo Pareja',\n",
       "   'Nikhil Shivakumar Nayak',\n",
       "   'Hao Wang',\n",
       "   'Krishnateja Killamsetty',\n",
       "   'Shivchander Sudalairaj',\n",
       "   'Wenlong Zhao',\n",
       "   'Seungwook Han',\n",
       "   'Abhishek Bhandwaldar',\n",
       "   'Guangxuan Xu',\n",
       "   'Kai Xu',\n",
       "   'Ligong Han',\n",
       "   'Luke Inglis',\n",
       "   'Akash Srivastava']},\n",
       " {'score': 41.217094,\n",
       "  'id': 'http://arxiv.org/abs/2408.11868v1',\n",
       "  'title': 'Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores',\n",
       "  'summary': 'This paper presents an approach to improve text embedding models through\\ncontrastive fine-tuning on small datasets augmented with expert scores. It\\nfocuses on enhancing semantic textual similarity tasks and addressing text\\nretrieval problems. The proposed method uses soft labels derived from\\nexpert-augmented scores to fine-tune embedding models, preserving their\\nversatility and ensuring retrieval capability is improved. The paper evaluates\\nthe method using a Q\\\\&A dataset from an online shopping website and eight\\nexpert models. Results show improved performance over a benchmark model across\\nmultiple metrics on various retrieval tasks from the massive text embedding\\nbenchmark (MTEB). The method is cost-effective and practical for real-world\\napplications, especially when labeled data is scarce.',\n",
       "  'author': ['Jun Lu', 'David Li', 'Bill Ding', 'Yu Kang']},\n",
       " {'score': 38.866642,\n",
       "  'id': 'http://arxiv.org/abs/1609.02531v1',\n",
       "  'title': 'Latest Datasets and Technologies Presented in the Workshop on Grasping and Manipulation Datasets',\n",
       "  'summary': 'This paper reports the activities and outcomes in the Workshop on Grasping\\nand Manipulation Datasets that was organized under the International Conference\\non Robotics and Automation (ICRA) 2016. The half day workshop was packed with\\nnine invited talks, 12 interactive presentations, and one panel discussion with\\nten panelists. This paper summarizes all the talks and presentations and recaps\\nwhat has been discussed in the panels session. This summary servers as a review\\nof recent developments in data collection in grasping and manipulation. Many of\\nthe presentations describe ongoing efforts or explorations that could be\\nachieved and fully available in a year or two. The panel discussion not only\\ncommented on the current approaches, but also indicates new directions and\\nfocuses. The workshop clearly displayed the importance of quality datasets in\\nrobotics and robotic grasping and manipulation field. Hopefully the workshop\\ncould motivate larger efforts to create big datasets that are comparable with\\nbig datasets in other communities such as computer vision.',\n",
       "  'author': ['Matteo Bianchi', 'Jeannette Bohg', 'Yu Sun']},\n",
       " {'score': 38.350567,\n",
       "  'id': 'http://arxiv.org/abs/2407.02960v2',\n",
       "  'title': 'ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets',\n",
       "  'summary': 'This work addresses the timely yet underexplored problem of performing\\ninference and finetuning of a proprietary LLM owned by a model provider entity\\non the confidential/private data of another data owner entity, in a way that\\nensures the confidentiality of both the model and the data. Hereby, the\\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\\nnovel, efficient and fully utility-preserving approach that combines a simple\\nyet effective obfuscation technique with an efficient usage of confidential\\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\\nna\\\\\"ive version of our approach to highlight the necessity of using random\\nmatrices with low condition numbers in our approach to reduce errors induced by\\nthe obfuscation.',\n",
       "  'author': ['Ahmed Frikha',\n",
       "   'Nassim Walha',\n",
       "   'Ricardo Mendes',\n",
       "   'Krishna Kanth Nakka',\n",
       "   'Xue Jiang',\n",
       "   'Xuebing Zhou']},\n",
       " {'score': 38.318386,\n",
       "  'id': 'http://arxiv.org/abs/2406.04383v2',\n",
       "  'title': 'Exploring the Latest LLMs for Leaderboard Extraction',\n",
       "  'summary': 'The rapid advancements in Large Language Models (LLMs) have opened new\\navenues for automating complex tasks in AI research. This paper investigates\\nthe efficacy of different LLMs-Mistral 7B, Llama-2, GPT-4-Turbo and GPT-4.o in\\nextracting leaderboard information from empirical AI research articles. We\\nexplore three types of contextual inputs to the models: DocTAET (Document\\nTitle, Abstract, Experimental Setup, and Tabular Information), DocREC (Results,\\nExperiments, and Conclusions), and DocFULL (entire document). Our comprehensive\\nstudy evaluates the performance of these models in generating (Task, Dataset,\\nMetric, Score) quadruples from research papers. The findings reveal significant\\ninsights into the strengths and limitations of each model and context type,\\nproviding valuable guidance for future AI research automation efforts.',\n",
       "  'author': ['Salomon Kabongo', \"Jennifer D'Souza\", 'Sören Auer']}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(query,top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719952e",
   "metadata": {},
   "source": [
    "Use an LLM to give an answer using as a context the most relevant papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f82236",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag(query,top_k=5,model=\"o4-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bdb7b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three main strands of very recent work on “small-data” fine-tuning of LLMs can be grouped as follows:\n",
      "\n",
      "1.  Supervised instruction-tuning of small (3 B–7 B) LLMs with hyper-parameter best-practices  \n",
      "    •  Large batch sizes + low learning rates often outperform the more common small-batch/high-LR recipes.  \n",
      "    •  Monitor early-stage training dynamics (gradient norms, loss curves) to kill bad runs and save computation.  \n",
      "    •  Simple learning-rate schedules and reduced warm-up are sufficient—no need for elaborate phased schedules.  \n",
      "    •  “Stacked” instruction mixing (train on all tasks at once) is as good as or better than multi-phase curricula, and is easier to implement.\n",
      "\n",
      "2.  Contrastive fine-tuning of embeddings on tiny labeled sets  \n",
      "    •  Build anchor/positive/negative pairs and use a contrastive loss to sharpen semantic similarity.  \n",
      "    •  Augment your small corpus with soft/expert-provided similarity scores so that the model “knows” graded relevance.  \n",
      "    •  This yields strong retrieval and downstream performance on tasks like Q & A or text search even when you only have hundreds of examples.\n",
      "\n",
      "3.  Privacy-preserving off-site fine-tuning of proprietary LLMs (ObfuscaTune)  \n",
      "    •  Model owner and data owner never see each other’s assets in the clear.  \n",
      "    •  Apply a light obfuscation (random low-condition matrices) to the model weights before sending them to the cloud.  \n",
      "    •  Perform the bulk of training off-site in standard compute, and confine only a small fraction (≈5 %) of parameters inside a trusted execution environment (TEE).  \n",
      "    •  At no point are the raw model weights or private data exposed, yet full fine-tuning utility is preserved.\n",
      "\n",
      "Taken together, these methods let practitioners with limited data and compute  (or with strict privacy requirements) successfully adapt LLMs.\n"
     ]
    }
   ],
   "source": [
    "print(answer['llm_answer'].choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abbb8042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_answer': ChatCompletion(id='chatcmpl-BpZ6WAKnXiWbGpbFFMxFkjopAlqQc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The three main strands of very recent work on “small-data” fine-tuning of LLMs can be grouped as follows:\\n\\n1.  Supervised instruction-tuning of small (3\\u2009B–7\\u2009B) LLMs with hyper-parameter best-practices  \\n    •  Large batch sizes + low learning rates often outperform the more common small-batch/high-LR recipes.  \\n    •  Monitor early-stage training dynamics (gradient norms, loss curves) to kill bad runs and save computation.  \\n    •  Simple learning-rate schedules and reduced warm-up are sufficient—no need for elaborate phased schedules.  \\n    •  “Stacked” instruction mixing (train on all tasks at once) is as good as or better than multi-phase curricula, and is easier to implement.\\n\\n2.  Contrastive fine-tuning of embeddings on tiny labeled sets  \\n    •  Build anchor/positive/negative pairs and use a contrastive loss to sharpen semantic similarity.  \\n    •  Augment your small corpus with soft/expert-provided similarity scores so that the model “knows” graded relevance.  \\n    •  This yields strong retrieval and downstream performance on tasks like Q\\u2009&\\u2009A or text search even when you only have hundreds of examples.\\n\\n3.  Privacy-preserving off-site fine-tuning of proprietary LLMs (ObfuscaTune)  \\n    •  Model owner and data owner never see each other’s assets in the clear.  \\n    •  Apply a light obfuscation (random low-condition matrices) to the model weights before sending them to the cloud.  \\n    •  Perform the bulk of training off-site in standard compute, and confine only a small fraction (≈5\\u2009%) of parameters inside a trusted execution environment (TEE).  \\n    •  At no point are the raw model weights or private data exposed, yet full fine-tuning utility is preserved.\\n\\nTaken together, these methods let practitioners with limited data and compute  (or with strict privacy requirements) successfully adapt LLMs.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1751629368, model='o4-mini-2025-04-16', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1342, prompt_tokens=1374, total_tokens=2716, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=896, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=1280))),\n",
       " 'sources': [{'score': 41.763905,\n",
       "   'id': 'http://arxiv.org/abs/2412.13337v1',\n",
       "   'title': 'Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs',\n",
       "   'summary': 'The rise of large language models (LLMs) has created a significant disparity:\\nindustrial research labs with their computational resources, expert teams, and\\nadvanced infrastructures, can effectively fine-tune LLMs, while individual\\ndevelopers and small organizations face barriers due to limited resources. In\\nthis paper, we aim to bridge this gap by presenting a comprehensive study on\\nsupervised fine-tuning of LLMs using instruction-tuning datasets spanning\\ndiverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B\\nparameters) for their cost-efficiency and accessibility. We explore various\\ntraining configurations and strategies across four open-source pre-trained\\nmodels. We provide detailed documentation of these configurations, revealing\\nfindings that challenge several common training practices, including\\nhyperparameter recommendations from TULU and phased training recommended by\\nOrca. Key insights from our work include: (i) larger batch sizes paired with\\nlower learning rates lead to improved model performance on benchmarks such as\\nMMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics,\\nsuch as lower gradient norms and higher loss values, are strong indicators of\\nbetter final model performance, enabling early termination of sub-optimal runs\\nand significant computational savings; (iii) through a thorough exploration of\\nhyperparameters like warmup steps and learning rate schedules, we provide\\nguidance for practitioners and find that certain simplifications do not\\ncompromise performance; and (iv) we observed no significant difference in\\nperformance between phased and stacked training strategies, but stacked\\ntraining is simpler and more sample efficient. With these findings holding\\nrobustly across datasets and models, we hope this study serves as a guide for\\npractitioners fine-tuning small LLMs and promotes a more inclusive environment\\nfor LLM research.',\n",
       "   'author': ['Aldo Pareja',\n",
       "    'Nikhil Shivakumar Nayak',\n",
       "    'Hao Wang',\n",
       "    'Krishnateja Killamsetty',\n",
       "    'Shivchander Sudalairaj',\n",
       "    'Wenlong Zhao',\n",
       "    'Seungwook Han',\n",
       "    'Abhishek Bhandwaldar',\n",
       "    'Guangxuan Xu',\n",
       "    'Kai Xu',\n",
       "    'Ligong Han',\n",
       "    'Luke Inglis',\n",
       "    'Akash Srivastava']},\n",
       "  {'score': 41.217094,\n",
       "   'id': 'http://arxiv.org/abs/2408.11868v1',\n",
       "   'title': 'Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores',\n",
       "   'summary': 'This paper presents an approach to improve text embedding models through\\ncontrastive fine-tuning on small datasets augmented with expert scores. It\\nfocuses on enhancing semantic textual similarity tasks and addressing text\\nretrieval problems. The proposed method uses soft labels derived from\\nexpert-augmented scores to fine-tune embedding models, preserving their\\nversatility and ensuring retrieval capability is improved. The paper evaluates\\nthe method using a Q\\\\&A dataset from an online shopping website and eight\\nexpert models. Results show improved performance over a benchmark model across\\nmultiple metrics on various retrieval tasks from the massive text embedding\\nbenchmark (MTEB). The method is cost-effective and practical for real-world\\napplications, especially when labeled data is scarce.',\n",
       "   'author': ['Jun Lu', 'David Li', 'Bill Ding', 'Yu Kang']},\n",
       "  {'score': 38.866642,\n",
       "   'id': 'http://arxiv.org/abs/1609.02531v1',\n",
       "   'title': 'Latest Datasets and Technologies Presented in the Workshop on Grasping and Manipulation Datasets',\n",
       "   'summary': 'This paper reports the activities and outcomes in the Workshop on Grasping\\nand Manipulation Datasets that was organized under the International Conference\\non Robotics and Automation (ICRA) 2016. The half day workshop was packed with\\nnine invited talks, 12 interactive presentations, and one panel discussion with\\nten panelists. This paper summarizes all the talks and presentations and recaps\\nwhat has been discussed in the panels session. This summary servers as a review\\nof recent developments in data collection in grasping and manipulation. Many of\\nthe presentations describe ongoing efforts or explorations that could be\\nachieved and fully available in a year or two. The panel discussion not only\\ncommented on the current approaches, but also indicates new directions and\\nfocuses. The workshop clearly displayed the importance of quality datasets in\\nrobotics and robotic grasping and manipulation field. Hopefully the workshop\\ncould motivate larger efforts to create big datasets that are comparable with\\nbig datasets in other communities such as computer vision.',\n",
       "   'author': ['Matteo Bianchi', 'Jeannette Bohg', 'Yu Sun']},\n",
       "  {'score': 38.350567,\n",
       "   'id': 'http://arxiv.org/abs/2407.02960v2',\n",
       "   'title': 'ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary LLMs on Private Datasets',\n",
       "   'summary': 'This work addresses the timely yet underexplored problem of performing\\ninference and finetuning of a proprietary LLM owned by a model provider entity\\non the confidential/private data of another data owner entity, in a way that\\nensures the confidentiality of both the model and the data. Hereby, the\\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\\nnovel, efficient and fully utility-preserving approach that combines a simple\\nyet effective obfuscation technique with an efficient usage of confidential\\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\\nna\\\\\"ive version of our approach to highlight the necessity of using random\\nmatrices with low condition numbers in our approach to reduce errors induced by\\nthe obfuscation.',\n",
       "   'author': ['Ahmed Frikha',\n",
       "    'Nassim Walha',\n",
       "    'Ricardo Mendes',\n",
       "    'Krishna Kanth Nakka',\n",
       "    'Xue Jiang',\n",
       "    'Xuebing Zhou']},\n",
       "  {'score': 38.318386,\n",
       "   'id': 'http://arxiv.org/abs/2406.04383v2',\n",
       "   'title': 'Exploring the Latest LLMs for Leaderboard Extraction',\n",
       "   'summary': 'The rapid advancements in Large Language Models (LLMs) have opened new\\navenues for automating complex tasks in AI research. This paper investigates\\nthe efficacy of different LLMs-Mistral 7B, Llama-2, GPT-4-Turbo and GPT-4.o in\\nextracting leaderboard information from empirical AI research articles. We\\nexplore three types of contextual inputs to the models: DocTAET (Document\\nTitle, Abstract, Experimental Setup, and Tabular Information), DocREC (Results,\\nExperiments, and Conclusions), and DocFULL (entire document). Our comprehensive\\nstudy evaluates the performance of these models in generating (Task, Dataset,\\nMetric, Score) quadruples from research papers. The findings reveal significant\\ninsights into the strengths and limitations of each model and context type,\\nproviding valuable guidance for future AI research automation efforts.',\n",
       "   'author': ['Salomon Kabongo', \"Jennifer D'Souza\", 'Sören Auer']}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766584e",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd464fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your generated questions\n",
    "eval_df = pd.read_csv('../data/arxiv_ground_truth_retrieval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "529a1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your RAG system\n",
    "def evaluate_rag(question, expected_paper_id):\n",
    "    # Run your RAG\n",
    "    results = search_papers(question, top_k=5)\n",
    "    \n",
    "    # Check if expected paper is in top results\n",
    "    retrieved_ids = [r['id'] for r in results]\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'expected_paper': expected_paper_id,\n",
    "        'retrieved_papers': retrieved_ids,\n",
    "        'hit_at_1': expected_paper_id == retrieved_ids[0] if retrieved_ids else False,\n",
    "        'hit_at_5': expected_paper_id in retrieved_ids\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "385cb689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@1: 0.62\n",
      "Hit@5: 0.75\n"
     ]
    }
   ],
   "source": [
    "sample_size=1000\n",
    "# Evaluate on sample\n",
    "sample_questions = eval_df.head(sample_size)\n",
    "eval_results = []\n",
    "\n",
    "for _, row in sample_questions.iterrows():\n",
    "    result = evaluate_rag(row['question'], row['paper_id'])\n",
    "    eval_results.append(result)\n",
    "\n",
    "# Calculate metrics\n",
    "hit_at_1 = sum([r['hit_at_1'] for r in eval_results]) / len(eval_results)\n",
    "hit_at_5 = sum([r['hit_at_5'] for r in eval_results]) / len(eval_results)\n",
    "\n",
    "print(f\"Hit@1: {hit_at_1:.2f}\")\n",
    "print(f\"Hit@5: {hit_at_5:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d3c10",
   "metadata": {},
   "source": [
    "## Optimize boosting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "628639e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06a6def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers_with_boost(query, title_boost=2.0, summary_boost=1.0, top_k=10):\n",
    "    \"\"\"Modified search function with configurable boost parameters\"\"\"\n",
    "    text_query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [f\"title^{title_boost}\", f\"summary^{summary_boost}\"],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k\n",
    "    }\n",
    "   \n",
    "    response = es.search(index=index_name, body=text_query)\n",
    "   \n",
    "    results = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        results.append({\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"id\": hit[\"_source\"][\"id\"],\n",
    "            \"title\": hit[\"_source\"][\"title\"],\n",
    "            \"summary\": hit[\"_source\"][\"summary\"],\n",
    "            \"author\": hit[\"_source\"][\"author\"]\n",
    "        })\n",
    "   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bd80e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_with_boost(question, expected_paper_id, title_boost=2.0, summary_boost=1.0):\n",
    "    \"\"\"Evaluate RAG with specific boost parameters\"\"\"\n",
    "    results = search_papers_with_boost(question, title_boost, summary_boost, top_k=5)\n",
    "    retrieved_ids = [r['id'] for r in results]\n",
    "   \n",
    "    return {\n",
    "        'question': question,\n",
    "        'expected_paper': expected_paper_id,\n",
    "        'retrieved_papers': retrieved_ids,\n",
    "        'hit_at_1': expected_paper_id == retrieved_ids[0] if retrieved_ids else False,\n",
    "        'hit_at_5': expected_paper_id in retrieved_ids,\n",
    "        'title_boost': title_boost,\n",
    "        'summary_boost': summary_boost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1be4edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_boost_params(eval_df, sample_size=50):\n",
    "    \"\"\"Grid search over boost parameters\"\"\"\n",
    "    # Define parameter ranges\n",
    "    title_boosts = [0.5, 1.0, 2.0, 3.0]\n",
    "    summary_boosts = [0.5, 1.0, 2.0, 3.0]\n",
    "    \n",
    "    sample_questions = eval_df.head(sample_size)\n",
    "    \n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    results_log = []\n",
    "    \n",
    "    print(f\"Testing {len(title_boosts) * len(summary_boosts)} parameter combinations...\")\n",
    "    \n",
    "    for title_boost, summary_boost in itertools.product(title_boosts, summary_boosts):\n",
    "        print(f\"Testing title_boost={title_boost}, summary_boost={summary_boost}\")\n",
    "        \n",
    "        eval_results = []\n",
    "        for _, row in sample_questions.iterrows():\n",
    "            result = evaluate_rag_with_boost(\n",
    "                row['question'], \n",
    "                row['paper_id'],\n",
    "                title_boost=title_boost,\n",
    "                summary_boost=summary_boost\n",
    "            )\n",
    "            eval_results.append(result)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        hit_at_1 = sum([r['hit_at_1'] for r in eval_results]) / len(eval_results)\n",
    "        hit_at_5 = sum([r['hit_at_5'] for r in eval_results]) / len(eval_results)\n",
    "        \n",
    "        # Use weighted score (adjust weights based on importance)\n",
    "        combined_score = 0.7 * hit_at_1 + 0.3 * hit_at_5\n",
    "        \n",
    "        results_log.append({\n",
    "            'title_boost': title_boost,\n",
    "            'summary_boost': summary_boost,\n",
    "            'hit_at_1': hit_at_1,\n",
    "            'hit_at_5': hit_at_5,\n",
    "            'combined_score': combined_score\n",
    "        })\n",
    "        \n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_params = (title_boost, summary_boost)\n",
    "            print(f\"New best: Hit@1={hit_at_1:.3f}, Hit@5={hit_at_5:.3f}, Score={combined_score:.3f}\")\n",
    "    \n",
    "    return best_params, results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c076a064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Grid Search Optimization ===\n",
      "Testing 16 parameter combinations...\n",
      "Testing title_boost=0.5, summary_boost=0.5\n",
      "New best: Hit@1=0.620, Hit@5=0.840, Score=0.686\n",
      "Testing title_boost=0.5, summary_boost=1.0\n",
      "New best: Hit@1=0.640, Hit@5=0.800, Score=0.688\n",
      "Testing title_boost=0.5, summary_boost=2.0\n",
      "Testing title_boost=0.5, summary_boost=3.0\n",
      "Testing title_boost=1.0, summary_boost=0.5\n",
      "Testing title_boost=1.0, summary_boost=1.0\n",
      "Testing title_boost=1.0, summary_boost=2.0\n",
      "Testing title_boost=1.0, summary_boost=3.0\n",
      "Testing title_boost=2.0, summary_boost=0.5\n",
      "Testing title_boost=2.0, summary_boost=1.0\n",
      "Testing title_boost=2.0, summary_boost=2.0\n",
      "Testing title_boost=2.0, summary_boost=3.0\n",
      "Testing title_boost=3.0, summary_boost=0.5\n",
      "Testing title_boost=3.0, summary_boost=1.0\n",
      "Testing title_boost=3.0, summary_boost=2.0\n",
      "Testing title_boost=3.0, summary_boost=3.0\n",
      "Best parameters: title_boost=0.5, summary_boost=1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Grid Search Optimization ===\")\n",
    "best_params, grid_results = grid_search_boost_params(eval_df, sample_size=50)\n",
    "print(f\"Best parameters: title_boost={best_params[0]}, summary_boost={best_params[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206ffcc",
   "metadata": {},
   "source": [
    "## RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2c2f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Structured evaluation result\"\"\"\n",
    "    relevance: str\n",
    "    explanation: str\n",
    "    confidence: Optional[float] = None\n",
    "    aspects: Optional[Dict[str, str]] = None\n",
    "\n",
    "class RAGJudge:\n",
    "    \"\"\"LLM-as-Judge evaluator for RAG systems\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, template_type=\"comprehensive\"):\n",
    "        self.llm_client = llm_client\n",
    "        self.template_type = template_type\n",
    "        self.templates = self._get_templates()\n",
    "        \n",
    "    def _get_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"Get evaluation prompt templates\"\"\"\n",
    "        templates = {\n",
    "            \"basic\": \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Evaluate the relevance of the generated answer to the given question.\n",
    "Classify as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Provide your evaluation in JSON format:\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip(),\n",
    "            \n",
    "            \"comprehensive\": \"\"\"\n",
    "You are an expert evaluator for a RAG system that retrieves and answers questions about academic papers.\n",
    "\n",
    "Your task is to evaluate the quality of the generated answer based on multiple criteria:\n",
    "\n",
    "1. **Relevance**: Does the answer address the question asked?\n",
    "2. **Accuracy**: Is the information provided factually correct?\n",
    "3. **Completeness**: Does the answer provide sufficient detail?\n",
    "4. **Clarity**: Is the answer well-structured and easy to understand?\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please provide your evaluation in JSON format:\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Accuracy\": \"INACCURATE\" | \"PARTLY_ACCURATE\" | \"ACCURATE\",\n",
    "  \"Completeness\": \"INCOMPLETE\" | \"PARTLY_COMPLETE\" | \"COMPLETE\",\n",
    "  \"Clarity\": \"UNCLEAR\" | \"PARTLY_CLEAR\" | \"CLEAR\",\n",
    "  \"Overall_Score\": 1-5,\n",
    "  \"Explanation\": \"[Detailed explanation of your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip(),\n",
    "            \n",
    "            \"with_context\": \"\"\"\n",
    "You are an expert evaluator for a RAG system that retrieves and answers questions about academic papers.\n",
    "\n",
    "Question: {question}\n",
    "Retrieved Papers: {retrieved_papers}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Evaluate the answer considering:\n",
    "1. **Relevance**: Does it answer the question?\n",
    "2. **Grounding**: Is it based on the retrieved papers?\n",
    "3. **Accuracy**: Is the information correct?\n",
    "4. **Citation**: Are sources properly referenced?\n",
    "\n",
    "Provide evaluation in JSON format:\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Grounding\": \"NOT_GROUNDED\" | \"PARTLY_GROUNDED\" | \"WELL_GROUNDED\",\n",
    "  \"Accuracy\": \"INACCURATE\" | \"PARTLY_ACCURATE\" | \"ACCURATE\",\n",
    "  \"Citation\": \"NO_CITATION\" | \"POOR_CITATION\" | \"GOOD_CITATION\",\n",
    "  \"Overall_Score\": 1-5,\n",
    "  \"Explanation\": \"[Detailed explanation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "        }\n",
    "        return templates\n",
    "    \n",
    "    def _clean_json_response(self, response: str) -> str:\n",
    "        \"\"\"Clean and extract JSON from LLM response\"\"\"\n",
    "        # Remove code blocks\n",
    "        response = re.sub(r'```json\\s*|\\s*```', '', response)\n",
    "        response = re.sub(r'```\\s*|\\s*```', '', response)\n",
    "        \n",
    "        # Find JSON-like content\n",
    "        json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json_match.group(0)\n",
    "        \n",
    "        return response.strip()\n",
    "    \n",
    "    def evaluate_single(self, question: str, answer: str, \n",
    "                       retrieved_papers: Optional[List[Dict]] = None) -> EvaluationResult:\n",
    "        \"\"\"Evaluate a single question-answer pair\"\"\"\n",
    "        \n",
    "        # Choose template based on available data\n",
    "        if retrieved_papers and self.template_type == \"with_context\":\n",
    "            template = self.templates[\"with_context\"]\n",
    "            papers_text = \"\\n\".join([\n",
    "                f\"- {paper.get('title', 'Unknown Title')}: {paper.get('summary', 'No summary')[:200]}...\"\n",
    "                for paper in retrieved_papers[:3]  # Limit to top 3 papers\n",
    "            ])\n",
    "            prompt = template.format(\n",
    "                question=question,\n",
    "                answer_llm=answer,\n",
    "                retrieved_papers=papers_text\n",
    "            )\n",
    "        else:\n",
    "            template = self.templates[self.template_type]\n",
    "            prompt = template.format(question=question, answer_llm=answer)\n",
    "        \n",
    "        try:\n",
    "            # Get LLM evaluation\n",
    "            raw_response = self.llm_client(prompt)\n",
    "            \n",
    "            # Clean and parse JSON\n",
    "            clean_response = self._clean_json_response(raw_response)\n",
    "            evaluation_dict = json.loads(clean_response)\n",
    "            \n",
    "            # Extract basic fields\n",
    "            relevance = evaluation_dict.get('Relevance', 'UNKNOWN')\n",
    "            explanation = evaluation_dict.get('Explanation', 'No explanation provided')\n",
    "            \n",
    "            # Extract additional fields for comprehensive evaluation\n",
    "            aspects = {}\n",
    "            if 'Accuracy' in evaluation_dict:\n",
    "                aspects['accuracy'] = evaluation_dict['Accuracy']\n",
    "            if 'Completeness' in evaluation_dict:\n",
    "                aspects['completeness'] = evaluation_dict['Completeness']\n",
    "            if 'Clarity' in evaluation_dict:\n",
    "                aspects['clarity'] = evaluation_dict['Clarity']\n",
    "            if 'Grounding' in evaluation_dict:\n",
    "                aspects['grounding'] = evaluation_dict['Grounding']\n",
    "            if 'Citation' in evaluation_dict:\n",
    "                aspects['citation'] = evaluation_dict['Citation']\n",
    "            \n",
    "            confidence = evaluation_dict.get('Overall_Score', None)\n",
    "            \n",
    "            return EvaluationResult(\n",
    "                relevance=relevance,\n",
    "                explanation=explanation,\n",
    "                confidence=confidence,\n",
    "                aspects=aspects if aspects else None\n",
    "            )\n",
    "            \n",
    "        except (json.JSONDecodeError, Exception) as e:\n",
    "            logger.error(f\"Error parsing evaluation: {e}\")\n",
    "            logger.error(f\"Raw response: {raw_response}\")\n",
    "            return EvaluationResult(\n",
    "                relevance=\"UNKNOWN\",\n",
    "                explanation=f\"Error parsing evaluation: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def evaluate_batch(self, eval_df: pd.DataFrame, \n",
    "                      rag_function,\n",
    "                      sample_size: int = 200,\n",
    "                      save_path: Optional[str] = None,\n",
    "                      rate_limit_delay: float = 0.1) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate a batch of questions\"\"\"\n",
    "        \n",
    "        # Sample data\n",
    "        if sample_size < len(eval_df):\n",
    "            df_sample = eval_df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            df_sample = eval_df.copy()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for idx, row in tqdm(df_sample.iterrows(), total=len(df_sample), desc=\"Evaluating\"):\n",
    "            question = row['question']\n",
    "            expected_paper_id = row.get('paper_id', None)\n",
    "            \n",
    "            try:\n",
    "                # Get RAG answer and retrieved papers\n",
    "                if hasattr(rag_function, '__call__'):\n",
    "                    # If rag_function returns both answer and papers\n",
    "                    try:\n",
    "                        rag_result = rag_function(question)\n",
    "                        if isinstance(rag_result, tuple):\n",
    "                            answer, retrieved_papers = rag_result\n",
    "                        else:\n",
    "                            answer = rag_result\n",
    "                            retrieved_papers = None\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error in RAG function: {e}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    answer = \"RAG function not callable\"\n",
    "                    retrieved_papers = None\n",
    "                \n",
    "                # Evaluate with LLM judge\n",
    "                evaluation = self.evaluate_single(question, answer, retrieved_papers)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'question': question,\n",
    "                    'expected_paper_id': expected_paper_id,\n",
    "                    'answer': answer,\n",
    "                    'relevance': evaluation.relevance,\n",
    "                    'explanation': evaluation.explanation,\n",
    "                    'confidence_score': evaluation.confidence,\n",
    "                }\n",
    "                \n",
    "                # Add aspect scores if available\n",
    "                if evaluation.aspects:\n",
    "                    result.update(evaluation.aspects)\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                # Rate limiting\n",
    "                if rate_limit_delay > 0:\n",
    "                    time.sleep(rate_limit_delay)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error evaluating question '{question}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # Save results\n",
    "        if save_path:\n",
    "            df_results.to_csv(save_path, index=False)\n",
    "            logger.info(f\"Results saved to {save_path}\")\n",
    "        \n",
    "        return df_results\n",
    "\n",
    "def analyze_evaluation_results(df_results: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze and summarize evaluation results\"\"\"\n",
    "    \n",
    "    analysis = {}\n",
    "    \n",
    "    # Basic relevance distribution\n",
    "    if 'relevance' in df_results.columns:\n",
    "        relevance_dist = df_results['relevance'].value_counts(normalize=True)\n",
    "        analysis['relevance_distribution'] = relevance_dist.to_dict()\n",
    "    \n",
    "    # Confidence score statistics\n",
    "    if 'confidence_score' in df_results.columns:\n",
    "        confidence_stats = df_results['confidence_score'].describe()\n",
    "        analysis['confidence_statistics'] = confidence_stats.to_dict()\n",
    "    \n",
    "    # Aspect analysis (if comprehensive evaluation)\n",
    "    aspects = ['accuracy', 'completeness', 'clarity', 'grounding', 'citation']\n",
    "    for aspect in aspects:\n",
    "        if aspect in df_results.columns:\n",
    "            aspect_dist = df_results[aspect].value_counts(normalize=True)\n",
    "            analysis[f'{aspect}_distribution'] = aspect_dist.to_dict()\n",
    "    \n",
    "    # Overall quality score\n",
    "    if 'confidence_score' in df_results.columns:\n",
    "        avg_score = df_results['confidence_score'].mean()\n",
    "        analysis['average_quality_score'] = avg_score\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def enhanced_rag_function(question: str, search_papers_func, llm_client) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"Enhanced RAG function that returns both answer and retrieved papers\"\"\"\n",
    "    \n",
    "    # Get relevant papers\n",
    "    retrieved_papers = search_papers_func(question, top_k=5)\n",
    "    \n",
    "    # Create context from papers\n",
    "    context = \"\"\n",
    "    for i, paper in enumerate(retrieved_papers[:3]):  # Use top 3 papers\n",
    "        context += f\"Paper {i+1}: {paper['title']}\\n\"\n",
    "        context += f\"Summary: {paper['summary'][:300]}...\\n\\n\"\n",
    "    \n",
    "    # Generate answer\n",
    "    answer_prompt = f\"\"\"\n",
    "Based on the following academic papers, answer the question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant Papers:\n",
    "{context}\n",
    "\n",
    "Please provide a comprehensive answer based on the information from these papers.\n",
    "\"\"\"\n",
    "    \n",
    "    answer = llm_client(answer_prompt)\n",
    "    \n",
    "    return answer, retrieved_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d037eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.026s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  10%|█         | 1/10 [00:07<01:11,  7.93s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.031s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|██        | 2/10 [00:19<01:21, 10.21s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.032s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|███       | 3/10 [00:31<01:15, 10.79s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.025s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  40%|████      | 4/10 [00:41<01:03, 10.66s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.031s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  50%|█████     | 5/10 [00:51<00:52, 10.42s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.022s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|██████    | 6/10 [01:02<00:41, 10.41s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.022s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  70%|███████   | 7/10 [01:13<00:32, 10.82s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.036s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|████████  | 8/10 [01:24<00:21, 10.65s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.027s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  90%|█████████ | 9/10 [01:32<00:09,  9.83s/it]INFO:elastic_transport.transport:POST http://localhost:9200/arxiv-papers/_search [status:200 duration:0.027s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|██████████| 10/10 [01:41<00:00, 10.19s/it]\n",
      "INFO:__main__:Results saved to rag_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a callable wrapper\n",
    "def llm_callable(prompt: str) -> str:\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # or your preferred model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Now use it with the judge\n",
    "judge = RAGJudge(llm_client=llm_callable, template_type=\"comprehensive\")\n",
    "\n",
    "# Create your RAG function\n",
    "def rag_function(question):\n",
    "    return enhanced_rag_function(question, search_papers, llm_callable)\n",
    "\n",
    "# Run evaluation\n",
    "rag_eval_results = judge.evaluate_batch(\n",
    "    eval_df=eval_df,\n",
    "    rag_function=rag_function,\n",
    "    sample_size=10, \n",
    "    save_path=\"../data/rag_evaluation_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d64c0316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_paper_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>completeness</th>\n",
       "      <th>clarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What limitations in existing embedding models ...</td>\n",
       "      <td>http://arxiv.org/abs/2007.06267v2</td>\n",
       "      <td>BoxE addresses several limitations in existing...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer directly addresses the qu...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the primary use cases of conversation...</td>\n",
       "      <td>http://arxiv.org/abs/2407.12004v1</td>\n",
       "      <td>Based on the summaries of the provided academi...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer directly addresses the qu...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What future research directions does the paper...</td>\n",
       "      <td>http://arxiv.org/abs/1311.0716v1</td>\n",
       "      <td>Based on the summaries of the three academic p...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer directly addresses the qu...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what ways does the report address the envir...</td>\n",
       "      <td>http://arxiv.org/abs/2310.03715v1</td>\n",
       "      <td>The provided summaries of the academic papers ...</td>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer addresses the question ab...</td>\n",
       "      <td>3</td>\n",
       "      <td>PARTLY_ACCURATE</td>\n",
       "      <td>PARTLY_COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What implications does the proposed method hav...</td>\n",
       "      <td>http://arxiv.org/abs/1212.4799v2</td>\n",
       "      <td>The proposed method, as discussed in the conte...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer directly addresses the im...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What technologies support the modular design o...</td>\n",
       "      <td>http://arxiv.org/abs/2003.00925v1</td>\n",
       "      <td>The modular design of the CAAI (Cognitive Arch...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer effectively addresses the...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How do the online procedures compare in terms ...</td>\n",
       "      <td>http://arxiv.org/abs/1007.0614v1</td>\n",
       "      <td>Based on the summaries of the provided academi...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer directly addresses the qu...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do the findings relate to the reliability ...</td>\n",
       "      <td>http://arxiv.org/abs/2403.14859v2</td>\n",
       "      <td>The findings from the three papers provide ins...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer directly addresses the qu...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In what ways do the authors suggest policymake...</td>\n",
       "      <td>http://arxiv.org/abs/2211.00065v1</td>\n",
       "      <td>Based on the summaries of the provided academi...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer directly addresses the qu...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What implications for future research on XAI i...</td>\n",
       "      <td>http://arxiv.org/abs/2211.06561v1</td>\n",
       "      <td>The implications for future research on Explai...</td>\n",
       "      <td>RELEVANT</td>\n",
       "      <td>The generated answer directly addresses the qu...</td>\n",
       "      <td>5</td>\n",
       "      <td>ACCURATE</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>CLEAR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What limitations in existing embedding models ...   \n",
       "1  What are the primary use cases of conversation...   \n",
       "2  What future research directions does the paper...   \n",
       "3  In what ways does the report address the envir...   \n",
       "4  What implications does the proposed method hav...   \n",
       "5  What technologies support the modular design o...   \n",
       "6  How do the online procedures compare in terms ...   \n",
       "7  How do the findings relate to the reliability ...   \n",
       "8  In what ways do the authors suggest policymake...   \n",
       "9  What implications for future research on XAI i...   \n",
       "\n",
       "                   expected_paper_id  \\\n",
       "0  http://arxiv.org/abs/2007.06267v2   \n",
       "1  http://arxiv.org/abs/2407.12004v1   \n",
       "2   http://arxiv.org/abs/1311.0716v1   \n",
       "3  http://arxiv.org/abs/2310.03715v1   \n",
       "4   http://arxiv.org/abs/1212.4799v2   \n",
       "5  http://arxiv.org/abs/2003.00925v1   \n",
       "6   http://arxiv.org/abs/1007.0614v1   \n",
       "7  http://arxiv.org/abs/2403.14859v2   \n",
       "8  http://arxiv.org/abs/2211.00065v1   \n",
       "9  http://arxiv.org/abs/2211.06561v1   \n",
       "\n",
       "                                              answer        relevance  \\\n",
       "0  BoxE addresses several limitations in existing...         RELEVANT   \n",
       "1  Based on the summaries of the provided academi...         RELEVANT   \n",
       "2  Based on the summaries of the three academic p...         RELEVANT   \n",
       "3  The provided summaries of the academic papers ...  PARTLY_RELEVANT   \n",
       "4  The proposed method, as discussed in the conte...         RELEVANT   \n",
       "5  The modular design of the CAAI (Cognitive Arch...         RELEVANT   \n",
       "6  Based on the summaries of the provided academi...         RELEVANT   \n",
       "7  The findings from the three papers provide ins...         RELEVANT   \n",
       "8  Based on the summaries of the provided academi...         RELEVANT   \n",
       "9  The implications for future research on Explai...         RELEVANT   \n",
       "\n",
       "                                         explanation  confidence_score  \\\n",
       "0  The generated answer directly addresses the qu...                 5   \n",
       "1  The generated answer directly addresses the qu...                 5   \n",
       "2  The generated answer directly addresses the qu...                 5   \n",
       "3  The generated answer addresses the question ab...                 3   \n",
       "4  The generated answer directly addresses the im...                 5   \n",
       "5  The generated answer effectively addresses the...                 5   \n",
       "6  The generated answer directly addresses the qu...                 5   \n",
       "7  The generated answer directly addresses the qu...                 5   \n",
       "8  The generated answer directly addresses the qu...                 5   \n",
       "9  The generated answer directly addresses the qu...                 5   \n",
       "\n",
       "          accuracy     completeness clarity  \n",
       "0         ACCURATE         COMPLETE   CLEAR  \n",
       "1         ACCURATE         COMPLETE   CLEAR  \n",
       "2         ACCURATE         COMPLETE   CLEAR  \n",
       "3  PARTLY_ACCURATE  PARTLY_COMPLETE   CLEAR  \n",
       "4         ACCURATE         COMPLETE   CLEAR  \n",
       "5         ACCURATE         COMPLETE   CLEAR  \n",
       "6         ACCURATE         COMPLETE   CLEAR  \n",
       "7         ACCURATE         COMPLETE   CLEAR  \n",
       "8         ACCURATE         COMPLETE   CLEAR  \n",
       "9         ACCURATE         COMPLETE   CLEAR  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03cf7e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_paper_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>completeness</th>\n",
       "      <th>clarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [question, expected_paper_id, answer, relevance, explanation, confidence_score, accuracy, completeness, clarity]\n",
       "Index: []"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_eval_results[rag_eval_results['relevance']=='NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81313c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Analysis:\n",
      "relevance_distribution: {'RELEVANT': 0.9, 'PARTLY_RELEVANT': 0.1}\n",
      "confidence_statistics: {'count': 10.0, 'mean': 4.8, 'std': 0.6324555320336759, 'min': 3.0, '25%': 5.0, '50%': 5.0, '75%': 5.0, 'max': 5.0}\n",
      "accuracy_distribution: {'ACCURATE': 0.9, 'PARTLY_ACCURATE': 0.1}\n",
      "completeness_distribution: {'COMPLETE': 0.9, 'PARTLY_COMPLETE': 0.1}\n",
      "clarity_distribution: {'CLEAR': 1.0}\n",
      "average_quality_score: 4.8\n"
     ]
    }
   ],
   "source": [
    "# Analyze results\n",
    "analysis = analyze_evaluation_results(rag_eval_results)\n",
    "print(\"Evaluation Analysis:\")\n",
    "for key, value in analysis.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different RAG configurations\n",
    "configurations = [\n",
    "    {\"name\": \"baseline\", \"title_boost\": 2.0, \"summary_boost\": 1.0},\n",
    "    {\"name\": \"optimized\", \"title_boost\": 3.5, \"summary_boost\": 1.5},\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "for config in configurations:\n",
    "    # Update search function with new parameters\n",
    "    rag_func = lambda q: enhanced_rag_function(\n",
    "        q, \n",
    "        lambda query, top_k: search_papers_with_boost(\n",
    "            query, config[\"title_boost\"], config[\"summary_boost\"], top_k\n",
    "        ),\n",
    "        llm_client\n",
    "    )\n",
    "    \n",
    "    results = judge.evaluate_batch(\n",
    "        eval_df=eval_df,\n",
    "        rag_function=rag_func,\n",
    "        sample_size=10,\n",
    "        save_path=f\"rag_eval_{config['name']}.csv\"\n",
    "    )\n",
    "    \n",
    "    comparison_results[config['name']] = analyze_evaluation_results(results)\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nConfiguration Comparison:\")\n",
    "for name, analysis in comparison_results.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    if 'average_quality_score' in analysis:\n",
    "        print(f\"  Average Quality Score: {analysis['average_quality_score']:.2f}\")\n",
    "    if 'relevance_distribution' in analysis:\n",
    "        relevant_pct = analysis['relevance_distribution'].get('RELEVANT', 0) * 100\n",
    "        print(f\"  Relevant Answers: {relevant_pct:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
