{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc7cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from google.cloud import bigquery\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d274e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73aa2d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN='cs-AI'\n",
    "GOOGLE_CLOUD_PROJECT='arxiv-trends'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4509dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabateri/anaconda3/envs/llm-project/lib/python3.11/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Initialize BQ client\n",
    "bq_client = bigquery.Client(project=GOOGLE_CLOUD_PROJECT)\n",
    "# Initialize OpenAI client\n",
    "llm_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86221cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bq_data(domain='cs-AI'):\n",
    "    domain_cleaned = domain.replace(\"-\", \"_\")\n",
    "    domain_cleaned = domain_cleaned.replace(\".\", \"_\")\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT id, title, summary, author\n",
    "    FROM `arxiv-trends.arxiv_papers.arxiv_papers_2000_2025_{domain_cleaned}`\n",
    "    WHERE summary IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = bq_client.query(sql_query)\n",
    "    results = query_job.result().to_dataframe()\n",
    "    return results\n",
    "\n",
    "raw_arxiv_df = get_bq_data(domain=DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1056496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates: 109203 -> 109203 rows\n"
     ]
    }
   ],
   "source": [
    "# Clean duplicates before bulk indexing\n",
    "arxiv_df = raw_arxiv_df.drop_duplicates(subset=['id'])\n",
    "print(f\"Removed duplicates: {len(raw_arxiv_df)} -> {len(arxiv_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12ecf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your arxiv_df to documents\n",
    "documents = arxiv_df.to_dict(orient='records')\n",
    "\n",
    "# Prompt template for generating questions about research papers\n",
    "prompt_template = \"\"\"\n",
    "You emulate a researcher or student using our ArXiv research assistant.\n",
    "Formulate 5 questions this user might ask based on the provided research paper.\n",
    "Make the questions specific to this paper's content, methods, findings, or applications.\n",
    "The paper summary should contain the answer to the questions, and the questions should\n",
    "be complete and research-focused. Use as few words as possible from the paper details.\n",
    "\n",
    "The paper:\n",
    "\n",
    "id: {id}\n",
    "title: {title}\n",
    "summary: {summary}\n",
    "author: {author}\n",
    "\n",
    "Create questions that would be naturally asked by someone researching this topic.\n",
    "Examples of good question types:\n",
    "- What methods does this paper propose for [specific problem]?\n",
    "- How does this approach compare to [related work]?\n",
    "- What are the main findings regarding [specific aspect]?\n",
    "- What datasets or experiments were used to validate [method]?\n",
    "- What are the limitations or future work suggested in [domain]?\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "{{\"questions\": [\"question1\", \"question2\", ..., \"question5\"]}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9efd7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(doc):\n",
    "    \"\"\"Generate questions for a single paper\"\"\"\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    \n",
    "    try:\n",
    "        response = llm_client.chat.completions.create(\n",
    "            model='gpt-4o-mini',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        json_response = response.choices[0].message.content\n",
    "        return json_response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating questions for paper {doc['id']}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd86669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with first paper...\n",
      "Sample questions generated:\n",
      "1. What is the main goal of the language Alog proposed in this paper?\n",
      "2. What algorithm does the paper introduce for computing answer sets in Alog?\n",
      "3. How does Alog's handling of aggregates compare to traditional ASP approaches?\n",
      "4. What properties of Alog are discussed in the paper?\n",
      "5. What future research directions are suggested based on the findings of this paper?\n",
      "\n",
      "Generating questions for 109203 papers...\n"
     ]
    }
   ],
   "source": [
    "# Test with first document\n",
    "print(\"Testing with first paper...\")\n",
    "test_prompt = prompt_template.format(**documents[0])\n",
    "test_response = llm_client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=[{\"role\": \"user\", \"content\": test_prompt}]\n",
    ")\n",
    "\n",
    "test_questions = json.loads(test_response.choices[0].message.content)\n",
    "print(\"Sample questions generated:\")\n",
    "for i, q in enumerate(test_questions['questions'], 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f5698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating questions for 109203 papers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8407fa2f8ed043839078c8d33781a537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse JSON for paper http://arxiv.org/abs/1405.3637v2: Invalid \\escape: line 1 column 85 (char 84)\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10\n",
    "# Generate questions for all papers\n",
    "print(f\"\\nGenerating questions for {len(documents)} papers...\")\n",
    "results = {}\n",
    "\n",
    "for doc in tqdm(documents[:sample_size]):  # Start with first 5 papers for testing\n",
    "    doc_id = doc['id']\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "    \n",
    "    questions_raw = generate_questions(doc)\n",
    "    if questions_raw is None:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        questions = json.loads(questions_raw)\n",
    "        results[doc_id] = questions['questions']\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON for paper {doc_id}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 45 question-paper pairs\n",
      "Sample results:\n",
      "1. Paper: http://arxiv.org/abs/1608.08262v1\n",
      "   Question: What alternative formalization of the Vicious Circle Principle is proposed in this paper?\n",
      "\n",
      "2. Paper: http://arxiv.org/abs/1608.08262v1\n",
      "   Question: How does Slog+ differ from the previously introduced language Alog in terms of set constructs?\n",
      "\n",
      "3. Paper: http://arxiv.org/abs/1608.08262v1\n",
      "   Question: In what specific scenarios does the formal semantics of Slog+ coincide with other known languages?\n",
      "\n",
      "4. Paper: http://arxiv.org/abs/1608.08262v1\n",
      "   Question: What implications does the incorporation of infinite sets have for knowledge representation in logic programming?\n",
      "\n",
      "5. Paper: http://arxiv.org/abs/1608.08262v1\n",
      "   Question: What are the key differences in the intuitive and formal semantics of Slog+ compared to its predecessors?\n",
      "\n",
      "Saved 45 questions to 'arxiv_ground_truth_retrieval.csv'\n",
      "\n",
      "DataFrame preview:\n",
      "                            paper_id  \\\n",
      "0  http://arxiv.org/abs/1608.08262v1   \n",
      "1  http://arxiv.org/abs/1608.08262v1   \n",
      "2  http://arxiv.org/abs/1608.08262v1   \n",
      "3  http://arxiv.org/abs/1608.08262v1   \n",
      "4  http://arxiv.org/abs/1608.08262v1   \n",
      "\n",
      "                                            question  \n",
      "0  What alternative formalization of the Vicious ...  \n",
      "1  How does Slog+ differ from the previously intr...  \n",
      "2  In what specific scenarios does the formal sem...  \n",
      "3  What implications does the incorporation of in...  \n",
      "4  What are the key differences in the intuitive ...  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to final format\n",
    "final_results = []\n",
    "\n",
    "for doc_id, questions in results.items():\n",
    "    for q in questions:\n",
    "        final_results.append((doc_id, q))\n",
    "\n",
    "print(f\"\\nGenerated {len(final_results)} question-paper pairs\")\n",
    "print(\"Sample results:\")\n",
    "for i in range(min(5, len(final_results))):\n",
    "    print(f\"{i+1}. Paper: {final_results[i][0]}\")\n",
    "    print(f\"   Question: {final_results[i][1]}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame and save\n",
    "df_results = pd.DataFrame(final_results, columns=['paper_id', 'question'])\n",
    "df_results.to_csv('../data/arxiv_ground_truth_retrieval.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(df_results)} questions to '../data/arxiv_ground_truth_retrieval.csv'\")\n",
    "print(\"\\nDataFrame preview:\")\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional evaluation: Generate questions for different domains/topics\n",
    "# def generate_domain_questions(n_papers=10):\n",
    "#     \"\"\"Generate questions focused on specific research domains\"\"\"\n",
    "    \n",
    "#     domain_prompt = \"\"\"\n",
    "#     You are a researcher in machine learning/AI. Based on the following research papers,\n",
    "#     generate 3 comparative or domain-specific questions that could be answered by \n",
    "#     analyzing multiple papers in this field.\n",
    "    \n",
    "#     Papers:\n",
    "#     {papers_context}\n",
    "    \n",
    "#     Generate questions like:\n",
    "#     - What are the different approaches to [specific problem] across these papers?\n",
    "#     - How do the methodologies compare between [paper A] and [paper B]?\n",
    "#     - What trends can be observed in [specific domain] research?\n",
    "    \n",
    "#     Provide output in JSON format:\n",
    "#     {{\"questions\": [\"question1\", \"question2\", \"question3\"]}}\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Take first n papers for domain questions\n",
    "#     sample_papers = documents[:n_papers]\n",
    "#     papers_context = \"\\n\\n\".join([\n",
    "#         f\"Paper {i+1}: {paper['title']}\\nSummary: {paper['summary'][:200]}...\"\n",
    "#         for i, paper in enumerate(sample_papers)\n",
    "#     ])\n",
    "    \n",
    "#     prompt = domain_prompt.format(papers_context=papers_context)\n",
    "    \n",
    "#     response = llm_client.chat.completions.create(\n",
    "#         model='gpt-4o-mini',\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#     )\n",
    "    \n",
    "#     return json.loads(response.choices[0].message.content)\n",
    "\n",
    "# # Generate domain-specific questions\n",
    "# domain_questions = generate_domain_questions()\n",
    "# print(\"\\nDomain-specific questions:\")\n",
    "# for i, q in enumerate(domain_questions['questions'], 1):\n",
    "#     print(f\"{i}. {q}\")\n",
    "\n",
    "# # Save domain questions separately\n",
    "# domain_df = pd.DataFrame(domain_questions['questions'], columns=['question'])\n",
    "# domain_df['question_type'] = 'domain_comparative'\n",
    "# domain_df.to_csv('arxiv_domain_questions.csv', index=False)\n",
    "\n",
    "# print(f\"\\nSaved {len(domain_df)} domain questions to 'arxiv_domain_questions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
