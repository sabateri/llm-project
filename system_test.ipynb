{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417b68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.rag import rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b95202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent work over the last year or so has driven self-supervision far beyond “vanilla” image-level pretext tasks.  In particular, four trends stand out:\n",
      "\n",
      "1.   Contrastive and hybrid pretext tasks  \n",
      "     •  Beyond SimCLR/MoCo/SwAV: people are now combining contrastive learning with other self-supervised objectives (e.g. masked‐prediction) to encourage richer features.  \n",
      "     •  NeuroNet (2404.17585) shows that a two-head network—with one head doing contrastive instance discrimination and another doing masked-signal reconstruction—can learn from raw EEG and match or exceed fully supervised sleep-stage classifiers with very few labels.  \n",
      "     •  A “Mamba” temporal context module further boosts performance by explicitly modeling epoch‐to‐epoch relationships in time-series data.  \n",
      "\n",
      "2.   Localized/region-level contrast for dense prediction  \n",
      "     •  In segmentation tasks one no longer treats the whole image as a single unit.  Localized Region Contrast (LRC, 2304.03406) uses superpixels to carve an image into semantically coherent regions and then applies contrastive sampling losses to those regions.  \n",
      "     •  This region-level pretraining yields large gains when fine-tuning on medical segmentation benchmarks, even with very few labeled masks.  \n",
      "\n",
      "3.   Semi-supervised + self-supervised pipelines for annotation-efficiency  \n",
      "     •  The S4MI framework (2311.10319) interleaves self-supervised pretraining on large pools of unlabeled medical scans with semi-supervised fine-tuning on small labeled subsets.  \n",
      "     •  In classification tasks S4MI outperforms fully supervised baselines; in segmentation it matches or beats them using only half the labels.  \n",
      "\n",
      "4.   Broader applications to non-vision modalities and foundation models  \n",
      "     •  In EEG research (2410.08224) self-supervised spatio-temporal encoders are now routinely used to build foundation models of brain signals—often in concert with graph neural networks or LLM-style architectures—to enable downstream tasks such as seizure detection, cognition monitoring or brain–computer interfaces.  \n",
      "     •  Surveys point to an emerging ecosystem of open-source EEG SSL toolkits, pre-trained checkpoints and benchmarks.  \n",
      "\n",
      "Taken together these developments mark a clear shift from simple proxy tasks toward:  \n",
      "– multi-task and hybrid objectives that merge contrastive, generative and masked-reconstruction losses;  \n",
      "– finer-granularity region-level discrimination for dense per-pixel predictions;  \n",
      "– turnkey self- and semi-supervised pipelines that dramatically reduce annotation burdens in domains like medical imaging;  \n",
      "– and the extension of SSL principles into time-series and foundation-model territory (EEG, GNNs, LLMs).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"What are the latest advancements in self-supervised learning?\"\n",
    "result = rag(query, top_k=5, model=\"o4-mini\")\n",
    "\n",
    "print(result[\"llm_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd478b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
